{
 "cells": [
  {
   "cell_type": "code",
   "id": "ee7f503228a27a79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:14:53.664596Z",
     "start_time": "2025-05-02T19:14:49.444515Z"
    }
   },
   "source": [
    "# add janus to os path\n",
    "import sys\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "import accelerate\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "janus_path = os.path.abspath(\"../Janus/janus\")\n",
    "sys.path.append(janus_path)\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:15:04.560309Z",
     "start_time": "2025-05-02T19:14:53.681717Z"
    }
   },
   "source": [
    "from abc import ABC\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from imagebind.models.imagebind_model import ImageBindModel\n",
    "from imagebind import data\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from Janus.janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from Janus.janus.models.processing_vlm import VLChatProcessorOutput\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mulham/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/mulham/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "/home/mulham/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mulham/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:602: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "527490705dcc9d36",
   "metadata": {},
   "source": [
    "# Audio Projection"
   ]
  },
  {
   "cell_type": "code",
   "id": "dcd1a4761cc629bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:15:04.630030Z",
     "start_time": "2025-05-02T19:15:04.624259Z"
    }
   },
   "source": [
    "class AudioProjection(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, sequal_len=32, scale_factor=2):\n",
    "        super(AudioProjection, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.sequal_len = sequal_len\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(input_dim, scale_factor * output_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(scale_factor * output_dim, sequal_len * output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.fc1(x)  # → [B, scale_factor * output_dim]\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)  # → [B, sequal_len * output_dim]\n",
    "        x = torch.reshape(x, (B, self.sequal_len, self.output_dim))\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "20d550bf896ee3f5",
   "metadata": {},
   "source": [
    "# Training config"
   ]
  },
  {
   "cell_type": "code",
   "id": "6adb6067b1dff347",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:15:04.701551Z",
     "start_time": "2025-05-02T19:15:04.671352Z"
    }
   },
   "source": [
    "class TrainConfig:  # copy from hw-multimodal-llm-solved need to be changed\n",
    "    log_level = \"DEBUG\"\n",
    "\n",
    "    # Training\n",
    "    num_epochs = 1\n",
    "    train_batch_size = 2\n",
    "    val_batch_size = 1\n",
    "    log_grad_norm = True\n",
    "    learning_rate = 1e-4\n",
    "    gradient_accumulation_steps = 1\n",
    "\n",
    "    evaluate_every_epoch_mod = 4\n",
    "    save_model_every_epoch_mod = 1\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "    # Model\n",
    "\n",
    "    # Projector\n",
    "    projector_input_dim = 1024\n",
    "\n",
    "    # Data\n",
    "    few_train_samples = None\n",
    "    few_val_samples = 100\n",
    "    dataloader_num_workers = 0\n",
    "\n",
    "    train_dataset_path = \"\"\n",
    "    audio_embeds_train_prefix = \"\"\n",
    "\n",
    "    val_dataset_path = \"\"\n",
    "    audio_embeds_val_prefix = \"\"\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "790d46bd401415af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:15:04.724700Z",
     "start_time": "2025-05-02T19:15:04.720770Z"
    }
   },
   "source": [
    "import yaml\n",
    "import argparse\n",
    "import pathlib\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import logging\n",
    "# import evaluate\n",
    "# import datasets\n",
    "from transformers.generation import GenerationConfig\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from tqdm.auto import tqdm\n",
    "# import wandb\n",
    "# from wandb import sdk as wandb_sdk\n",
    "# import accelerate\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "24a09a48901bc57f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:15:45.868824Z",
     "start_time": "2025-05-02T19:15:04.773730Z"
    }
   },
   "source": [
    "model_path = f\"deepseek-ai/Janus-Pro-1B\"\n",
    "\n",
    "prompt = \"Abstract art for representing emotions\"\n",
    "\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "input_ids = torch.LongTensor(input_ids)\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True,\n",
    ")\n",
    "vl_gpt.language_model.config._attn_implementation = 'eager'\n",
    "\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Some kwargs in processor config are unused and will not have any effect: mask_prompt, add_special_token, image_tag, ignore_id, num_image_tokens, sft_format. \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "e3885e00a9b486b9",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "id": "dcd9f065a8fb6d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:15:47.543432Z",
     "start_time": "2025-05-02T19:15:46.625929Z"
    }
   },
   "source": [
    "matched_df_path = \"../data/notebooks/matched_dataset_concat.pkl\"\n",
    "matched_df = pd.read_pickle(matched_df_path)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "a9f8dbaeb36c8eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:15:48.664636Z",
     "start_time": "2025-05-02T19:15:48.654264Z"
    }
   },
   "source": [
    "class ImageAudioDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.dataframe[\"image_path\"] = self.dataframe[\"image_path\"].apply(\n",
    "            lambda x: os.path.abspath(x.replace(\"\\\\\", \"/\")))\n",
    "        self.dataframe[\"audio_path\"] = self.dataframe[\"audio_path\"].apply(\n",
    "            lambda x: os.path.abspath(x.replace(\"\\\\\", \"/\")))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        image_path = self.dataframe.iloc[idx][\"image_path\"]\n",
    "        audio_path = self.dataframe.iloc[idx][\"audio_path\"]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = ToTensor()(image)\n",
    "            # resize to 384x384\n",
    "            image = nn.functional.interpolate(image.unsqueeze(0), size=(384, 384), mode='bilinear', align_corners=False)\n",
    "\n",
    "        image_embedding = self.dataframe.iloc[idx][\"image_embedding\"]\n",
    "        music_embedding = self.dataframe.iloc[idx][\"music_embedding\"]\n",
    "        res = {\n",
    "            \"audio_path\": audio_path,\n",
    "            \"image\": image,\n",
    "            \"music_embedding\": music_embedding,\n",
    "            \"image_embedding\": image_embedding,\n",
    "        }\n",
    "        return res"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "976199c3603f371f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:15:48.720326Z",
     "start_time": "2025-05-02T19:15:48.675451Z"
    }
   },
   "source": [
    "dataset = ImageAudioDataset(matched_df)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "e6c9f237c7ec4214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:15:49.392929Z",
     "start_time": "2025-05-02T19:15:49.350836Z"
    }
   },
   "source": [
    "from transformers.models.bart.modeling_bart import shift_tokens_right  # similar utility\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def shift_image_tokens(image_ids: torch.Tensor):\n",
    "\n",
    "    image_embeds_shifted = shift_tokens_right(\n",
    "        image_ids,\n",
    "        pad_token_id=vl_chat_processor.pad_id,\n",
    "        decoder_start_token_id=vl_chat_processor.image_start_id\n",
    "    )\n",
    "    return image_embeds_shifted\n",
    "\n",
    "\n",
    "def get_collate_fn(gen_model: MultiModalityCausalLM, validation=False):\n",
    "    def collate_fn(items):\n",
    "        result = dict()\n",
    "        with torch.no_grad():\n",
    "            images = torch.stack([item[\"image\"] for item in items], dim=1)\n",
    "            quant, _, info = gen_model.gen_vision_model.encode(\n",
    "                images.squeeze(0).to(dtype=torch.bfloat16).cuda())  # torch.Size([1, 3, 384, 384])\n",
    "            B, C, Hq, Wq = quant.shape\n",
    "            _, _, min_encoding_indices = info\n",
    "            image_ids = min_encoding_indices.view(B, Hq * Wq)\n",
    "            gen_embeds = gen_model.prepare_gen_img_embeds(image_ids)\n",
    "\n",
    "        result[\"image_ids\"] = image_ids.squeeze(-1)\n",
    "        result[\"image_gen_embeds\"] = gen_embeds\n",
    "        result[\"music_embedding\"] = torch.stack([torch.from_numpy(item[\"music_embedding\"]) for item in items], dim=0)\n",
    "        result[\"images\"] = images\n",
    "\n",
    "        return result\n",
    "\n",
    "    return collate_fn"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "562e51ac48274d0b",
   "metadata": {},
   "source": [
    "### Dataloader example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42c45f842f676283",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T09:59:23.546945Z",
     "start_time": "2025-05-01T09:59:13.842056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 384, 384])\n",
      "torch.Size([2, 576, 2048])\n",
      "torch.Size([2, 576])\n",
      "torch.Size([2, 1024])\n",
      "Epoch 1, Batch 1: Processed 0 samples\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(TrainConfig.num_epochs):\n",
    "    dataloader = DataLoader(dataset, batch_size=TrainConfig.train_batch_size, shuffle=True,\n",
    "                            num_workers=TrainConfig.dataloader_num_workers,\n",
    "                            collate_fn=get_collate_fn(vl_gpt)\n",
    "                            )\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Example: Access data from the batch\n",
    "        print(batch[\"images\"].shape)\n",
    "        print(batch[\"image_gen_embeds\"].shape)\n",
    "        print(batch[\"image_ids\"].shape)\n",
    "        print(batch[\"music_embedding\"].shape)\n",
    "\n",
    "        # Simulate training step (e.g., forward pass, loss computation, backward pass)\n",
    "        print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}: Processed {len([])} samples\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce856e12d58c7d64",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8ad54ba833024d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T10:21:36.036181Z",
     "start_time": "2025-05-01T10:21:35.998288Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(accelerator: accelerate.Accelerator, model: MultiModalityCausalLM, projection: AudioProjection,\n",
    "               optimizer, train_dataloader: DataLoader, epoch, criterion, last_validation_bleu=0.0, train_config=None):\n",
    "    model.eval()\n",
    "    projection.train()\n",
    "    progress_bar = tqdm(range(len(train_dataloader)), desc=\"Epoch {}\".format(epoch))\n",
    "    for batch in train_dataloader:\n",
    "        with accelerator.accumulate(projection):\n",
    "            audio_input = projection(batch[\"music_embedding\"]).to(torch.bfloat16)\n",
    "            image_gen_embeds = batch[\"image_gen_embeds\"].to(torch.bfloat16)\n",
    "            image_ids = batch[\"image_ids\"]\n",
    "\n",
    "            # add sys prompt embeds to input\n",
    "            input_embeds = torch.concat([audio_input, image_gen_embeds], dim=1)\n",
    "            print(input_embeds.shape)\n",
    "            print(input_embeds.type)\n",
    "            outputs = model.language_model.model(inputs_embeds=input_embeds, use_cache=False, past_key_values=None,\n",
    "                                          decoder_input_ids=1)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            logits = model.gen_head(hidden_states)\n",
    "            logits = logits.permute(0, 2, 1)\n",
    "            shifted_image_ids = shift_image_tokens(image_ids)\n",
    "            print(\n",
    "                shifted_image_ids.shape, shifted_image_ids.dtype, logits.shape, sep=\"\\n\"\n",
    "            )\n",
    "            loss = criterion(logits[:, :, -576:].cpu(), image_ids.cpu())\n",
    "            model.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_description(f'Epoch={epoch} Loss={loss.item():.3f}')\n",
    "\n",
    "            step_metrics = {\"train_loss\": loss.item(), \"epoch\": epoch}\n",
    "            if train_config.log_grad_norm:\n",
    "                for name, parameter in projection.named_parameters():\n",
    "                    if parameter.grad is not None:\n",
    "                        parameter_grad_norm = parameter.grad.norm(2).item()\n",
    "                    else:\n",
    "                        parameter_grad_norm = 0.0\n",
    "                    step_metrics[f'grad_norm_{name}'] = parameter_grad_norm\n",
    "\n",
    "    return"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Val loop",
   "id": "d23ca8fb15e8d968"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:57:40.822183Z",
     "start_time": "2025-05-02T19:57:40.806971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def val_loop( model: MultiModalityCausalLM, processor: VLChatProcessor, projection: AudioProjection,\n",
    "              val_dataloader: DataLoader, epoch=1, no_loss=False, captioning_metrics=None):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=processor.pad_id)\n",
    "\n",
    "    sumloss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    generations = []\n",
    "    target_generations = []\n",
    "\n",
    "    # gen_params = {\n",
    "    #     \"do_sample\": False,\n",
    "    #     \"early_stopping\": True,\n",
    "    #     \"num_beams\": 3,\n",
    "    #     \"repetition_penalty\": 2.5,\n",
    "    #     \"remove_invalid_values\": True,\n",
    "    #     \"eos_token_id\": processor.eos_token_id,\n",
    "    #     \"pad_token_id\": processor.eos_token_id,\n",
    "    #     \"forced_eos_token_id\": processor.eos_token_id,\n",
    "    #     \"use_cache\": True,\n",
    "    #     \"no_repeat_ngram_size\": 4,\n",
    "    #     \"num_return_sequences\": 1,\n",
    "    #     \"cfg_weight\": 5,\n",
    "    #     \"temperature\":1\n",
    "    # }\n",
    "    cfg_weight = 5\n",
    "    temperature = 1\n",
    "    img_size = 384\n",
    "    patch_size = 16\n",
    "    genconfig = GenerationConfig.from_model_config(model.language_model.config)\n",
    "\n",
    "    model.eval()\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        batch_input_ids = batch['image_ids'].to(model.device)\n",
    "        caption_legth = batch_input_ids.shape[1]\n",
    "\n",
    "        music_embedding = projection(batch[\"music_embedding\"]).to(torch.bfloat16).cuda()\n",
    "        image_gen_embeds = batch[\"image_gen_embeds\"].to(torch.bfloat16)\n",
    "        batch_input_embeds = music_embedding\n",
    "        input_embeds = torch.concat([batch_input_embeds, image_gen_embeds], dim=1)\n",
    "\n",
    "        if not no_loss:\n",
    "            # outputs = model.language_model.model(inputs_embeds=input_embeds, use_cache=False, past_key_values=None,\n",
    "            #                               decoder_input_ids=1)\n",
    "            # hidden_states = outputs.last_hidden_state\n",
    "            hidden_states = torch.rand(input_embeds.shape).to(torch.bfloat16).cuda()\n",
    "            logits = model.gen_head(hidden_states)\n",
    "            logits = logits.permute(0, 2, 1)\n",
    "\n",
    "            shifted_image_ids = shift_image_tokens(batch_input_ids)\n",
    "\n",
    "            loss = criterion(logits[:, :, -576:].cpu(), batch_input_ids.cpu())\n",
    "            sumloss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        parallel_size = music_embedding.shape[0]\n",
    "        tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()\n",
    "        unconditional_tokens = torch.zeros((1, input_embeds.shape[-2]), dtype=torch.int).cuda()\n",
    "        print(input_embeds.shape)\n",
    "        print(unconditional_tokens.shape)\n",
    "        unconditional_tokens[0, 1:-1] = processor.pad_id\n",
    "        assert unconditional_tokens.shape[0] == 1\n",
    "        # assert unconditional_tokens.shape[-1] == input_embeds.shape[-2]\n",
    "\n",
    "        unconditional_embeds = model.language_model.get_input_embeddings()(unconditional_tokens)\n",
    "        assert unconditional_embeds.shape[-1] == input_embeds.shape[-1]\n",
    "\n",
    "        gen_input_embeds = torch.zeros((input_embeds.shape[0] * 2, *input_embeds.shape[1:]), dtype=torch.bfloat16).cuda()\n",
    "        print(gen_input_embeds.shape)\n",
    "        for i in range(parallel_size*2):\n",
    "            if i % 2 != 0:\n",
    "                gen_input_embeds[i] = unconditional_embeds\n",
    "            else:\n",
    "                gen_input_embeds[i] = input_embeds[i//2]\n",
    "\n",
    "        generated_tokens = torch.zeros((parallel_size, batch_input_ids.shape[-1]), dtype=torch.int).cuda()\n",
    "\n",
    "        for i in range(batch_input_ids.shape[-1]):\n",
    "            # outputs = model.language_model.model(inputs_embeds=gen_input_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)\n",
    "            # hidden_states = outputs.last_hidden_state\n",
    "            hidden_states = torch.rand((4, 578, 2048), dtype=torch.bfloat16).cuda()\n",
    "            logits = model.gen_head(hidden_states[:, -1, :])\n",
    "            logit_cond = logits[0::2, :]\n",
    "            logit_uncond = logits[1::2, :]\n",
    "\n",
    "            logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_tokens[:, i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "            next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n",
    "            img_embeds = model.prepare_gen_img_embeds(next_token)\n",
    "            inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "        dec = model.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int), shape=[parallel_size, 8, img_size//patch_size, img_size//patch_size])\n",
    "        dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "        dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n",
    "\n",
    "        visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)\n",
    "        visual_img[:, :, :] = dec\n",
    "\n",
    "        os.makedirs('generated_samples', exist_ok=True)\n",
    "        for i in range(parallel_size):\n",
    "            save_path = os.path.join('generated_samples', \"val_img_{}.jpg\".format(i))\n",
    "            Image.fromarray(visual_img[i]).save(save_path)\n"
   ],
   "id": "3ff6f90c3583bed",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:59:39.318319Z",
     "start_time": "2025-05-02T19:57:41.660035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "proj = AudioProjection(1024, 2048, scale_factor=2, sequal_len=2)\n",
    "val_dataloader = DataLoader(dataset, batch_size=TrainConfig.train_batch_size, shuffle=True,\n",
    "                            num_workers=TrainConfig.dataloader_num_workers,\n",
    "                            collate_fn=get_collate_fn(vl_gpt)\n",
    "                            )\n",
    "val_loop(vl_gpt, vl_chat_processor, proj, val_dataloader )"
   ],
   "id": "469f2e8a0bc3a724",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/2743 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ccfc9935a2f44aa3904404ed090274a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 578, 2048])\n",
      "torch.Size([1, 578])\n",
      "torch.Size([4, 578, 2048])\n",
      "torch.Size([2, 578, 2048])\n",
      "torch.Size([1, 578])\n",
      "torch.Size([4, 578, 2048])\n",
      "torch.Size([2, 578, 2048])\n",
      "torch.Size([1, 578])\n",
      "torch.Size([4, 578, 2048])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m proj \u001B[38;5;241m=\u001B[39m AudioProjection(\u001B[38;5;241m1024\u001B[39m, \u001B[38;5;241m2048\u001B[39m, scale_factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, sequal_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m      2\u001B[0m val_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39mTrainConfig\u001B[38;5;241m.\u001B[39mtrain_batch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      3\u001B[0m                             num_workers\u001B[38;5;241m=\u001B[39mTrainConfig\u001B[38;5;241m.\u001B[39mdataloader_num_workers,\n\u001B[1;32m      4\u001B[0m                             collate_fn\u001B[38;5;241m=\u001B[39mget_collate_fn(vl_gpt)\n\u001B[1;32m      5\u001B[0m                             )\n\u001B[0;32m----> 6\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvl_gpt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvl_chat_processor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[44], line 83\u001B[0m, in \u001B[0;36mval_loop\u001B[0;34m(model, processor, projection, val_dataloader, epoch, no_loss, captioning_metrics)\u001B[0m\n\u001B[1;32m     78\u001B[0m generated_tokens \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros((parallel_size, batch_input_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mint)\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(batch_input_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]):\n\u001B[1;32m     81\u001B[0m     \u001B[38;5;66;03m# outputs = model.language_model.model(inputs_embeds=gen_input_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)\u001B[39;00m\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;66;03m# hidden_states = outputs.last_hidden_state\u001B[39;00m\n\u001B[0;32m---> 83\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrand\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m578\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2048\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbfloat16\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m     84\u001B[0m     logits \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgen_head(hidden_states[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :])\n\u001B[1;32m     85\u001B[0m     logit_cond \u001B[38;5;241m=\u001B[39m logits[\u001B[38;5;241m0\u001B[39m::\u001B[38;5;241m2\u001B[39m, :]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d1f6cb8ec2e7563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T10:21:37.149979Z",
     "start_time": "2025-05-01T10:21:37.138636Z"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "\n",
    "def train(\n",
    "        model: MultiModalityCausalLM,\n",
    "        projection: AudioProjection,\n",
    "        processor: VLChatProcessor,\n",
    "        train_dataloader: DataLoader,\n",
    "        val_dataloader: DataLoader,\n",
    "        train_config: TrainConfig,\n",
    "        device_placement=True,\n",
    "        ):\n",
    "    trainable_parameters = list(projection.parameters())\n",
    "    optimizer = Adam(trainable_parameters, lr=train_config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    accelerator = accelerate.Accelerator(device_placement=device_placement)\n",
    "    accelerator.gradient_accumulation_steps = train_config.gradient_accumulation_steps\n",
    "\n",
    "    model, projection, optimizer, train_dataloader, val_dataloader = accelerator.prepare(model, projection, optimizer, train_dataloader, val_dataloader)\n",
    "    # captioning_metrics = evaluate.combine(\n",
    "    #     [\n",
    "    #         evaluate.load(\"bleu\", keep_in_memory=True),\n",
    "    #         evaluate.load(\"rouge\", keep_in_memory=True),\n",
    "    #         evaluate.load(\"meteor\", keep_in_memory=True),\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "    best_validation_bleu = 0.0\n",
    "    last_validation_bleu = 0.0\n",
    "\n",
    "    for epoch in range(train_config.num_epochs):\n",
    "        train_loop(accelerator, model,projection, optimizer, train_dataloader, epoch=epoch, criterion=criterion, last_validation_bleu=last_validation_bleu, train_config=train_config)\n",
    "\n",
    "    #     if epoch % train_config.evaluate_every_epoch_mod == 0:\n",
    "    #         validation_metrics = val_loop(model, tokenizer, val_dataloader, epoch=epoch, captioning_metrics=captioning_metrics)\n",
    "    #         logger.info(f\"validation metrics {validation_metrics}\")\n",
    "    #\n",
    "    #         last_validation_bleu = validation_metrics['validation/evaluate_bleu']\n",
    "    #         metric_logger.log(validation_metrics)\n",
    "    #\n",
    "    #         if last_validation_bleu > best_validation_bleu:\n",
    "    #             best_validation_bleu = last_validation_bleu\n",
    "    #\n",
    "    #             base_path_for_best_model = pathlib.Path(f\"data/models/{metric_logger.name}/best/\")\n",
    "    #             save_model(train_config=train_config, model=model, path=base_path_for_best_model)\n",
    "    #\n",
    "    #     if epoch % train_config.save_model_every_epoch_mod == 0:\n",
    "    #         base_path_for_model = pathlib.Path(f\"data/models/{metric_logger.name}/last/\")\n",
    "    #         save_model(train_config=train_config, model=model, path=base_path_for_model)\n",
    "    #\n",
    "    # base_path_for_model = pathlib.Path(f\"data/models/{metric_logger.name}/last/\")\n",
    "    # save_model(train_config=train_config, model=model, path=base_path_for_model)\n",
    "\n",
    "\n",
    "def freeze_model(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ddd6b42c826e5c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T10:13:53.625522Z",
     "start_time": "2025-05-01T10:12:59.364631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb4b92de66e4906bd6841866d5f56bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/2743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 608, 2048])\n",
      "<built-in method type of Tensor object at 0x7f4280717fb0>\n",
      "torch.Size([2, 576])\n",
      "torch.int64\n",
      "torch.Size([2, 16384, 608])\n",
      "torch.Size([2, 608, 2048])\n",
      "<built-in method type of Tensor object at 0x7f42807dbf60>\n",
      "torch.Size([2, 576])\n",
      "torch.int64\n",
      "torch.Size([2, 16384, 608])\n",
      "torch.Size([2, 608, 2048])\n",
      "<built-in method type of Tensor object at 0x7f4280570270>\n",
      "torch.Size([2, 576])\n",
      "torch.int64\n",
      "torch.Size([2, 16384, 608])\n",
      "torch.Size([2, 608, 2048])\n",
      "<built-in method type of Tensor object at 0x7f4281057330>\n",
      "torch.Size([2, 576])\n",
      "torch.int64\n",
      "torch.Size([2, 16384, 608])\n",
      "torch.Size([2, 608, 2048])\n",
      "<built-in method type of Tensor object at 0x7f4280594810>\n",
      "torch.Size([2, 576])\n",
      "torch.int64\n",
      "torch.Size([2, 16384, 608])\n",
      "torch.Size([2, 608, 2048])\n",
      "<built-in method type of Tensor object at 0x7f4281207920>\n",
      "torch.Size([2, 576])\n",
      "torch.int64\n",
      "torch.Size([2, 16384, 608])\n",
      "torch.Size([2, 608, 2048])\n",
      "<built-in method type of Tensor object at 0x7f4280d265c0>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m proj \u001B[38;5;241m=\u001B[39m AudioProjection(\u001B[38;5;241m1024\u001B[39m, \u001B[38;5;241m2048\u001B[39m, scale_factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m      2\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39mTrainConfig\u001B[38;5;241m.\u001B[39mtrain_batch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      3\u001B[0m                             num_workers\u001B[38;5;241m=\u001B[39mTrainConfig\u001B[38;5;241m.\u001B[39mdataloader_num_workers,\n\u001B[1;32m      4\u001B[0m                             collate_fn\u001B[38;5;241m=\u001B[39mget_collate_fn(vl_gpt)\n\u001B[1;32m      5\u001B[0m                             )\n\u001B[0;32m----> 7\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvl_gpt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprojection\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprocessor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvl_chat_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTrainConfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[18], line 33\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, projection, processor, train_dataloader, val_dataloader, train_config, device_placement)\u001B[0m\n\u001B[1;32m     30\u001B[0m last_validation_bleu \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(train_config\u001B[38;5;241m.\u001B[39mnum_epochs):\n\u001B[0;32m---> 33\u001B[0m     \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mprojection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlast_validation_bleu\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlast_validation_bleu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[17], line 16\u001B[0m, in \u001B[0;36mtrain_loop\u001B[0;34m(accelerator, model, projection, optimizer, train_dataloader, epoch, criterion, last_validation_bleu, train_config)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(input_embeds\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(input_embeds\u001B[38;5;241m.\u001B[39mtype)\n\u001B[0;32m---> 16\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlanguage_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_embeds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlast_hidden_state\n\u001B[1;32m     19\u001B[0m logits \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgen_head(hidden_states)\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:601\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001B[0m\n\u001B[1;32m    589\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    590\u001B[0m         decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    591\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    598\u001B[0m         position_embeddings,\n\u001B[1;32m    599\u001B[0m     )\n\u001B[1;32m    600\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 601\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    602\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    604\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mflash_attn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    611\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    613\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    615\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:343\u001B[0m, in \u001B[0;36mLlamaDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    340\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_layernorm(hidden_states)\n\u001B[1;32m    342\u001B[0m \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[0;32m--> 343\u001B[0m hidden_states, self_attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    354\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    356\u001B[0m \u001B[38;5;66;03m# Fully Connected\u001B[39;00m\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:299\u001B[0m, in \u001B[0;36mLlamaAttention.forward\u001B[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001B[0m\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    297\u001B[0m         attention_interface \u001B[38;5;241m=\u001B[39m ALL_ATTENTION_FUNCTIONS[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_attn_implementation]\n\u001B[0;32m--> 299\u001B[0m attn_output, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[43mattention_interface\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    300\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    301\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    302\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkey_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    303\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalue_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    304\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    305\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention_dropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    306\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscaling\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscaling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    307\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    308\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    310\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m*\u001B[39minput_shape, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m    311\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mo_proj(attn_output)\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:226\u001B[0m, in \u001B[0;36meager_attention_forward\u001B[0;34m(module, query, key, value, attention_mask, scaling, dropout, **kwargs)\u001B[0m\n\u001B[1;32m    223\u001B[0m key_states \u001B[38;5;241m=\u001B[39m repeat_kv(key, module\u001B[38;5;241m.\u001B[39mnum_key_value_groups)\n\u001B[1;32m    224\u001B[0m value_states \u001B[38;5;241m=\u001B[39m repeat_kv(value, module\u001B[38;5;241m.\u001B[39mnum_key_value_groups)\n\u001B[0;32m--> 226\u001B[0m attn_weights \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_states\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m scaling\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    228\u001B[0m     causal_mask \u001B[38;5;241m=\u001B[39m attention_mask[:, :, :, : key_states\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m]]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "proj = AudioProjection(1024, 2048, scale_factor=2)\n",
    "train_dataloader = DataLoader(dataset, batch_size=TrainConfig.train_batch_size, shuffle=True,\n",
    "                            num_workers=TrainConfig.dataloader_num_workers,\n",
    "                            collate_fn=get_collate_fn(vl_gpt)\n",
    "                            )\n",
    "\n",
    "train(\n",
    "    model=vl_gpt,\n",
    "    projection=proj,\n",
    "    processor=vl_chat_processor,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=None,\n",
    "    train_config=TrainConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7134bb31549d058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T10:22:13.005633Z",
     "start_time": "2025-05-01T10:22:12.966881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty cuda\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8737d61e65409f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
