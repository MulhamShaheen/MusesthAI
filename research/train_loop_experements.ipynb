{
 "cells": [
  {
   "cell_type": "code",
   "id": "ee7f503228a27a79",
   "metadata": {},
   "source": [
    "# add janus to os path\n",
    "import sys\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "import accelerate\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "janus_path = os.path.abspath(\"../Janus/janus\")\n",
    "sys.path.append(janus_path)\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "from abc import ABC\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from imagebind.models.imagebind_model import ImageBindModel\n",
    "from imagebind import data\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from Janus.janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from Janus.janus.models.processing_vlm import VLChatProcessorOutput\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "527490705dcc9d36",
   "metadata": {},
   "source": [
    "# Audio Projection"
   ]
  },
  {
   "cell_type": "code",
   "id": "dcd1a4761cc629bb",
   "metadata": {},
   "source": [
    "class AudioProjection(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, sequal_len=32, scale_factor=2):\n",
    "        super(AudioProjection, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.sequal_len = sequal_len\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(input_dim, scale_factor * output_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(scale_factor * output_dim, sequal_len * output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.fc1(x)  # → [B, scale_factor * output_dim]\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)  # → [B, sequal_len * output_dim]\n",
    "        x = torch.reshape(x, (B, self.sequal_len, self.output_dim))\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ImprovedAudioProjection(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, seq_len=32, num_layers=2, dropout=0.1, activation='gelu', use_l2=True,\n",
    "                 scale_up: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.use_l2 = use_l2\n",
    "        self.scale_up = scale_up\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU(0.2)\n",
    "        elif activation == 'swish':\n",
    "            self.activation = nn.SiLU()\n",
    "        else:\n",
    "            self.activation = nn.GELU()\n",
    "\n",
    "        # Sequential Layers\n",
    "        self.projection_layers = nn.ModuleList()\n",
    "        in_dim = input_dim\n",
    "        for i in range(num_layers):\n",
    "            out_dim_layer = output_dim if i == num_layers - 1 else 2 * output_dim\n",
    "            self.projection_layers.append(nn.Linear(in_dim, out_dim_layer))\n",
    "            self.projection_layers.append(nn.LayerNorm(out_dim_layer))\n",
    "            if i < num_layers - 1:\n",
    "                self.projection_layers.append(self.activation)\n",
    "                self.projection_layers.append(nn.Dropout(dropout))\n",
    "            in_dim = out_dim_layer\n",
    "\n",
    "        self.final_reshape = nn.Linear(output_dim, seq_len * output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Apply sequential linear and activation layers\n",
    "        for layer in self.projection_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Final projection and reshape\n",
    "        x = self.final_reshape(x)\n",
    "        x = x.reshape(B, self.seq_len, self.output_dim)\n",
    "        if self.use_l2:\n",
    "            x = F.normalize(x, p = 2, dim = -1)\n",
    "        return x * self.scale_up\n"
   ],
   "id": "59a52913bb3a582d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming ImprovedAudioProjection is already imported\n",
    "\n",
    "# Define input and output dimensions\n",
    "input_dim = 1024\n",
    "output_dim = 2048\n",
    "seq_len = 24\n",
    "num_layers = 3\n",
    "dropout = 0.1\n",
    "activation = 'gelu'\n",
    "\n",
    "# Initialize the model\n",
    "model = ImprovedAudioProjection(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    seq_len,\n",
    "    num_layers=3,\n",
    "    dropout=0.1,\n",
    "    activation='gelu',\n",
    "    use_l2=True,\n",
    "    scale_up=1,\n",
    ").cuda()\n",
    "\n",
    "# if os.path.exists(f\"../proj_seq_{seq_len}.pt\"):\n",
    "#     print(\"Loading model from checkpoint...\")\n",
    "#     model.load_state_dict(torch.load(f\"../proj_seq_{seq_len}.pt\"))\n",
    "\n",
    "# Create random input tensor\n",
    "batch_size = 4\n",
    "input_tensor = torch.rand(batch_size, input_dim).cuda()\n",
    "\n",
    "# Pass the input through the model\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n"
   ],
   "id": "81fc20ae834c9e6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('../runs/model_visualization_experiment')\n",
    "writer.add_graph(model, input_tensor)\n",
    "writer.close()"
   ],
   "id": "561054d58d506954",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "logits = torch.rand(2, 576, 2048)\n",
    "\n",
    "logits_norm = torch.linalg.norm(logits.float(), 'fro', dim=(-2, -1)).mean().item()\n",
    "\n",
    "audio_input_float = torch.rand(2, 64, 2048).float()\n",
    "\n",
    "audio_input_mean = torch.mean(audio_input_float).item()\n",
    "audio_input_std = torch.std(audio_input_float).item()\n",
    "audio_input_norm = torch.linalg.norm(audio_input_float, 'fro', dim=(-2,-1))"
   ],
   "id": "5f2252f9c3046b31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "audio_input_std",
   "id": "bd934716471a28f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "output.min()",
   "id": "56e421260cb9cae7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "20d550bf896ee3f5",
   "metadata": {},
   "source": [
    "# Training config"
   ]
  },
  {
   "cell_type": "code",
   "id": "6adb6067b1dff347",
   "metadata": {},
   "source": [
    "class TrainConfig:  # copy from hw-multimodal-llm-solved need to be changed\n",
    "    log_level = \"DEBUG\"\n",
    "\n",
    "    # Training\n",
    "    num_epochs = 1\n",
    "    train_batch_size = 2\n",
    "    val_batch_size = 1\n",
    "    log_grad_norm = True\n",
    "    learning_rate = 1e-4\n",
    "    gradient_accumulation_steps = 1\n",
    "\n",
    "    evaluate_every_epoch_mod = 4\n",
    "    save_model_every_epoch_mod = 1\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "    # Model\n",
    "\n",
    "    # Projector\n",
    "    projector_input_dim = 1024\n",
    "\n",
    "    # Data\n",
    "    few_train_samples = None\n",
    "    few_val_samples = 100\n",
    "    dataloader_num_workers = 0\n",
    "\n",
    "    train_dataset_path = \"\"\n",
    "    audio_embeds_train_prefix = \"\"\n",
    "\n",
    "    val_dataset_path = \"\"\n",
    "    audio_embeds_val_prefix = \"\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "790d46bd401415af",
   "metadata": {},
   "source": [
    "import yaml\n",
    "import argparse\n",
    "import pathlib\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import logging\n",
    "# import evaluate\n",
    "# import datasets\n",
    "from transformers.generation import GenerationConfig\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from tqdm.auto import tqdm\n",
    "# import wandb\n",
    "# from wandb import sdk as wandb_sdk\n",
    "# import accelerate\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24a09a48901bc57f",
   "metadata": {},
   "source": [
    "model_path = f\"deepseek-ai/Janus-Pro-1B\"\n",
    "\n",
    "prompt = \"Abstract art for representing emotions\"\n",
    "\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "input_ids = torch.LongTensor(input_ids)\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True,\n",
    ")\n",
    "vl_gpt.language_model.config._attn_implementation = 'eager'\n",
    "\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"User\",\n",
    "        \"content\": \"Art for representing emotions\",\n",
    "    },\n",
    "    {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "    conversations=conversation,\n",
    "    sft_format=vl_chat_processor.sft_format,\n",
    "    system_prompt=\"\",\n",
    ")\n",
    "prompt = sft_format + vl_chat_processor.image_start_tag\n",
    "\n",
    "prompt_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "prompt_ids = torch.LongTensor(prompt_ids).cuda()\n",
    "prompt_embeds = vl_gpt.language_model.get_input_embeddings()(prompt_ids).to(torch.bfloat16).unsqueeze(0)"
   ],
   "id": "6425d4ad8b8cdbe3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f098d718dce4a24e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_sample(music_embedding, batched_prompt_embeds, image_token_num_per_image, processor, model, file_prefix):\n",
    "    cfg_weight = 5\n",
    "    temperature = 1\n",
    "    img_size = 384\n",
    "    patch_size = 16\n",
    "\n",
    "    parallel_size = music_embedding.shape[0]\n",
    "    conditional_embeds = torch.concat([music_embedding, batched_prompt_embeds], dim=1)\n",
    "    unconditional_tokens = torch.zeros((1, conditional_embeds.shape[-2]), dtype=torch.int).cuda()\n",
    "    unconditional_tokens[0, 1:-1] = processor.pad_id\n",
    "    unconditional_embeds = model.language_model.get_input_embeddings()(unconditional_tokens)\n",
    "    gen_input_embeds = torch.zeros(\n",
    "        (conditional_embeds.shape[0] * 2, conditional_embeds.shape[1], conditional_embeds.shape[2]),\n",
    "        dtype=torch.bfloat16).cuda()\n",
    "\n",
    "    for i in range(parallel_size * 2):\n",
    "        if i % 2 != 0:\n",
    "            gen_input_embeds[i] = unconditional_embeds\n",
    "        else:\n",
    "            gen_input_embeds[i] = conditional_embeds[i // 2]\n",
    "\n",
    "    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()\n",
    "    inputs_embeds = gen_input_embeds\n",
    "    for i in range(image_token_num_per_image):\n",
    "        outputs = model.language_model.model(inputs_embeds=inputs_embeds, use_cache=True,\n",
    "                                             past_key_values=outputs.past_key_values if i != 0 else None)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        logits = model.gen_head(hidden_states[:, -1, :])\n",
    "        logit_cond = logits[0::2, :]\n",
    "        logit_uncond = logits[1::2, :]\n",
    "\n",
    "        logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated_tokens[:, i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n",
    "        img_embeds = model.prepare_gen_img_embeds(next_token)\n",
    "        inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "    dec = model.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int),\n",
    "                                             shape=[parallel_size, 8, img_size // patch_size,\n",
    "                                                    img_size // patch_size])\n",
    "    dec = dec.to(torch.float32)\n",
    "    dec = torch.clamp((dec + 1) / 2 * 255, min=0, max=255)\n",
    "\n",
    "    visual_img = dec.cpu().numpy().transpose(0, 2, 3, 1).astype(np.uint8)\n",
    "\n",
    "    os.makedirs('generated_samples', exist_ok=True)\n",
    "    for i in range(parallel_size):\n",
    "        save_path = os.path.join('generated_samples', \"{}_{}.jpg\".format(file_prefix, i))\n",
    "        Image.fromarray(visual_img[i]).save(save_path)\n",
    "\n",
    "    visual_img = dec.to(torch.uint8)\n",
    "    return visual_img"
   ],
   "id": "4419f096ce099166",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8da9c80fcf1f2623",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "music_embeds = torch.rand(1, 32, 2048, dtype=torch.bfloat16).cuda()\n",
    "with torch.no_grad():\n",
    "    generate_sample(music_embeds, prompt_embeds, 576, vl_chat_processor, vl_gpt, \"image\")"
   ],
   "id": "4e2785c2c1dc81cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3885e00a9b486b9",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "id": "dcd9f065a8fb6d3b",
   "metadata": {},
   "source": [
    "matched_df_path = \"../data/notebooks/matched_dataset_concat.pkl\"\n",
    "matched_df = pd.read_pickle(matched_df_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9f8dbaeb36c8eed",
   "metadata": {},
   "source": [
    "class ImageAudioDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.dataframe[\"image_path\"] = self.dataframe[\"image_path\"].apply(\n",
    "            lambda x: os.path.abspath(x.replace(\"\\\\\", \"/\")))\n",
    "        self.dataframe[\"audio_path\"] = self.dataframe[\"audio_path\"].apply(\n",
    "            lambda x: os.path.abspath(x.replace(\"\\\\\", \"/\")))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        image_path = self.dataframe.iloc[idx][\"image_path\"]\n",
    "        audio_path = self.dataframe.iloc[idx][\"audio_path\"]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = ToTensor()(image)\n",
    "            # resize to 384x384\n",
    "            image = nn.functional.interpolate(image.unsqueeze(0), size=(384, 384), mode='bilinear', align_corners=False)\n",
    "\n",
    "        image_embedding = self.dataframe.iloc[idx][\"image_embedding\"]\n",
    "        music_embedding = self.dataframe.iloc[idx][\"music_embedding\"]\n",
    "        res = {\n",
    "            \"audio_path\": audio_path,\n",
    "            \"image\": image,\n",
    "            \"music_embedding\": music_embedding,\n",
    "            \"image_embedding\": image_embedding,\n",
    "        }\n",
    "        return res"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "976199c3603f371f",
   "metadata": {},
   "source": [
    "\n",
    "dataset = ImageAudioDataset(matched_df.sample(n=10, random_state=42))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res = dataset[0]\n",
    "res[\"image\"].shape, res[\"music_embedding\"].shape, res[\"image_embedding\"].shape"
   ],
   "id": "990e00daf866fff2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6c9f237c7ec4214",
   "metadata": {},
   "source": [
    "from transformers.models.bart.modeling_bart import shift_tokens_right  # similar utility\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Shifts input_ids one token to the right, prepending the decoder_start_token_id.\n",
    "    This is a common way to create decoder_input_ids for teacher-forcing.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_ids, torch.Tensor):\n",
    "        raise TypeError(\"input_ids should be a torch.Tensor\")\n",
    "    if not isinstance(pad_token_id, int):\n",
    "        raise TypeError(\"pad_token_id should be an int\")\n",
    "    if not isinstance(decoder_start_token_id, int):\n",
    "        raise TypeError(\"decoder_start_token_id should be an int\")\n",
    "\n",
    "    shifted_input_ids = torch.full_like(input_ids, pad_token_id)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    return shifted_input_ids.to(torch.long)\n",
    "\n",
    "\n",
    "\n",
    "def get_collate_fn(gen_model: MultiModalityCausalLM):\n",
    "    def collate_fn(items):\n",
    "        result = dict()\n",
    "        with torch.no_grad():\n",
    "            images = torch.cat([item[\"image\"] for item in items], dim=0)\n",
    "            quant, _, info = gen_model.gen_vision_model.encode(\n",
    "                images.to(dtype=torch.bfloat16).cuda())\n",
    "            B, C, Hq, Wq = quant.shape\n",
    "            _, _, min_encoding_indices = info\n",
    "            image_ids = min_encoding_indices.view(B, Hq * Wq)\n",
    "            image_ids = shift_tokens_right(image_ids, 0, 1).cuda()\n",
    "            gen_embeds = gen_model.prepare_gen_img_embeds(image_ids)\n",
    "\n",
    "        result[\"image_ids\"] = image_ids.squeeze(-1)\n",
    "        result[\"image_gen_embeds\"] = gen_embeds\n",
    "        result[\"music_embedding\"] = torch.stack([torch.from_numpy(item[\"music_embedding\"]) for item in items], dim=0)\n",
    "        result[\"images\"] = images\n",
    "\n",
    "        return result\n",
    "\n",
    "    return collate_fn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "562e51ac48274d0b",
   "metadata": {},
   "source": [
    "### Dataloader example"
   ]
  },
  {
   "cell_type": "code",
   "id": "42c45f842f676283",
   "metadata": {},
   "source": [
    "for epoch in range(TrainConfig.num_epochs):\n",
    "    dataloader = DataLoader(dataset, batch_size=TrainConfig.train_batch_size, shuffle=True,\n",
    "                            num_workers=TrainConfig.dataloader_num_workers,\n",
    "                            collate_fn=get_collate_fn(vl_gpt)\n",
    "                            )\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Example: Access data from the batch\n",
    "        print(batch[\"images\"].shape)\n",
    "        print(batch[\"image_gen_embeds\"].shape)\n",
    "        print(batch[\"image_ids\"].shape)\n",
    "        print(batch[\"music_embedding\"].shape)\n",
    "\n",
    "        # Simulate training step (e.g., forward pass, loss computation, backward pass)\n",
    "        print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}: Processed {len([])} samples\")\n",
    "        break\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch[\"image_ids\"].dtype\n",
    "s_image_ids = shift_tokens_right(batch[\"image_ids\"], 0, 0)\n",
    "gen_embeds = vl_gpt.prepare_gen_img_embeds(s_image_ids)"
   ],
   "id": "15f32e144b399467",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "batch[\"image_ids\"].max()",
   "id": "b99b1262d7f8ce3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vl_gpt.config.gen_head_config",
   "id": "6a7e0711bb1cff1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "batch[\"image_gen_embeds\"].max()",
   "id": "c3d4a5ebbb900124",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce856e12d58c7d64",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8ad54ba833024d9",
   "metadata": {},
   "source": [
    "def train_loop(accelerator, model, processor, projection, optimizer, train_dataloader, epoch, criterion, train_config,\n",
    "               metric_logger: None,\n",
    "               mock_run: bool = False):\n",
    "    model.eval()\n",
    "    projection.train()\n",
    "    progress_bar = tqdm(range(len(train_dataloader)), desc=f\"Epoch {epoch}\")\n",
    "    total_loss = 0\n",
    "    prompt_embeds = None\n",
    "\n",
    "    if train_config.sys_prompt is not None:\n",
    "        prompt_ids = processor.tokenizer.encode(train_config.sys_prompt)\n",
    "        prompt_ids = torch.LongTensor(prompt_ids).cuda()\n",
    "        prompt_embeds = model.language_model.get_input_embeddings()(prompt_ids).to(torch.bfloat16).unsqueeze(0)\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        with accelerator.accumulate(projection):\n",
    "            B = audio_input.shape[0]\n",
    "\n",
    "            music_embedding = batch[\"music_embedding\"].to(model.device)\n",
    "            audio_input = projection(music_embedding).to(torch.bfloat16)\n",
    "            audio_attention_mask = torch.ones(B, audio_input.shape[1], dtype=torch.long)\n",
    "\n",
    "            image_ids = batch[\"image_ids\"]\n",
    "            image_gen_embeds = batch[\"image_gen_embeds\"].to(torch.bfloat16)\n",
    "            image_attention_mask = torch.ones(B, image_gen_embeds.shape[1], dtype=torch.long)\n",
    "            image_attention_mask[:, 0] = 0\n",
    "\n",
    "            if prompt_embeds is not None:\n",
    "                batched_prompt_embeds = prompt_embeds.repeat(B, 1, 1)\n",
    "                prompt_attention_mask = torch.ones(B, batched_prompt_embeds.shape[1], dtype=torch.long)\n",
    "                input_embeds = torch.concat([batched_prompt_embeds, audio_input, image_gen_embeds], dim=1)\n",
    "                attention_mask = torch.concat([prompt_attention_mask, audio_attention_mask, image_attention_mask], dim=1)\n",
    "\n",
    "            else:\n",
    "                input_embeds = torch.concat([audio_input, image_gen_embeds], dim=1)\n",
    "                attention_mask = torch.concat([audio_attention_mask, image_attention_mask], dim=1)\n",
    "\n",
    "            if mock_run:\n",
    "                hidden_states = torch.rand(input_embeds.shape).cuda().to(torch.bfloat16)\n",
    "            else:\n",
    "                outputs = model.language_model.model(inputs_embeds=input_embeds, use_cache=False, past_key_values=None,\n",
    "                                                     decoder_input_ids=1)\n",
    "                hidden_states = outputs.last_hidden_state\n",
    "\n",
    "            logits = model.gen_head(hidden_states)\n",
    "            logits = logits.permute(0, 2, 1)\n",
    "            loss = criterion(logits[:, :, -576:], image_ids)\n",
    "            total_loss += loss.item()\n",
    "            step_metrics = {\"train_loss\": loss.item(), \"epoch\": epoch}\n",
    "\n",
    "            model.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_description(f'Epoch={epoch} Loss={loss.item():.3f}')\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    return average_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Shifts input_ids one token to the right, prepending the decoder_start_token_id.\n",
    "    This is a common way to create decoder_input_ids for teacher-forcing.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_ids, torch.Tensor):\n",
    "        raise TypeError(\"input_ids should be a torch.Tensor\")\n",
    "    if not isinstance(pad_token_id, int):\n",
    "        raise TypeError(\"pad_token_id should be an int\")\n",
    "    if not isinstance(decoder_start_token_id, int):\n",
    "        raise TypeError(\"decoder_start_token_id should be an int\")\n",
    "\n",
    "    shifted_input_ids = torch.full_like(input_ids, pad_token_id)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    return shifted_input_ids"
   ],
   "id": "5cdd3fb9a40b770d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "B = batch[\"music_embedding\"].shape[0]\n",
    "audio_input = torch.rand(B, 64, 2048).to(torch.bfloat16).cuda()\n",
    "prompt_embeds = torch.rand(1, 64, 2048).to(torch.bfloat16).cuda()\n",
    "\n",
    "music_embedding = batch[\"music_embedding\"].to(vl_gpt.device)\n",
    "audio_attention_mask = torch.ones(B, audio_input.shape[1], dtype=torch.long)\n",
    "\n",
    "image_ids = batch[\"image_ids\"]\n",
    "image_gen_embeds = batch[\"image_gen_embeds\"].to(torch.bfloat16)\n",
    "image_attention_mask = torch.ones(B, image_gen_embeds.shape[1], dtype=torch.long)\n",
    "image_attention_mask[:, 0] = 0\n",
    "\n",
    "batched_prompt_embeds = prompt_embeds.repeat(B, 1, 1)\n",
    "prompt_attention_mask = torch.ones(B, batched_prompt_embeds.shape[1], dtype=torch.long)\n",
    "input_embeds = torch.concat([batched_prompt_embeds, audio_input, image_gen_embeds], dim=1)\n",
    "\n",
    "attention_mask = torch.concat([prompt_attention_mask, audio_attention_mask, image_attention_mask], dim=1)\n",
    "unmasked_outputs = vl_gpt.language_model.model(inputs_embeds=input_embeds, use_cache=False, past_key_values=None)"
   ],
   "id": "f663b0a179b4e293",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "outputs.last_hidden_state.shape",
   "id": "80392942176a9dab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get the difference between unmasked_outputs.last_hidden_state and outputs.last_hidden_state\n",
    "difference = torch.norm(unmasked_outputs.last_hidden_state - outputs.last_hidden_state)\n",
    "difference"
   ],
   "id": "117752d2c93af3b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "im = torch.rand([1, 50, 2048]).cuda().to(torch.bfloat16)\n",
    "\n",
    "outputs = vl_gpt.language_model.model(inputs_embeds=im, attention_mask=torch.zeros(1,50,dtype=torch.long).cuda(), suse_cache=False, past_key_values=None)\n",
    "outputs.last_hidden_state.detach()\n",
    " # 0.4258,  0.1797,  1.1797,  ...,  0.3301, -0.4980,  0.1426"
   ],
   "id": "aec86be39671a77f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Val loop",
   "id": "d23ca8fb15e8d968"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore"
   ],
   "id": "add9f01426fd4a13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def val_loop(model, processor, projection, val_dataloader, metrics: dict = None, epoch=1, no_loss=False,\n",
    "             generate_freq=0):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=processor.pad_id)\n",
    "    sumloss = 0\n",
    "    num_batches = 0\n",
    "    if not metrics:\n",
    "        fid = FrechetInceptionDistance(feature=2048).to(model.device)\n",
    "        inception_score = InceptionScore(feature='logits_unbiased', splits=10).to(model.device)\n",
    "    else:\n",
    "        fid = metrics['fid']\n",
    "        inception_score = metrics['inception_score']\n",
    "\n",
    "    # hardcoded values\n",
    "    cfg_weight = 5\n",
    "    temperature = 1\n",
    "    img_size = 384\n",
    "    patch_size = 16\n",
    "\n",
    "    if not generate_freq:\n",
    "        generate_freq = len(val_dataloader) + 1\n",
    "\n",
    "    model.eval()\n",
    "    projection.eval()\n",
    "    i = 0\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        batch_input_ids = batch['image_ids'].to(model.device)\n",
    "        music_embedding = batch[\"music_embedding\"].to(model.device)\n",
    "        music_embedding = projection(music_embedding).to(torch.bfloat16)\n",
    "        image_gen_embeds = batch[\"image_gen_embeds\"].to(model.device).to(torch.bfloat16)\n",
    "\n",
    "        input_embeds = torch.concat([music_embedding, image_gen_embeds], dim=1)\n",
    "\n",
    "        if not no_loss:\n",
    "            # outputs = model.language_model.model(inputs_embeds=input_embeds, use_cache=False, past_key_values=None,\n",
    "            #                                      decoder_input_ids=1)\n",
    "            # hidden_states = outputs.last_hidden_state\n",
    "            hidden_states = torch.rand((1, 608, 2048)).cuda().to(torch.bfloat16)\n",
    "            logits = model.gen_head(hidden_states)\n",
    "            logits = logits.permute(0, 2, 1)\n",
    "            loss = criterion(logits[:, :, -576:].cpu(), batch_input_ids.cpu())\n",
    "            sumloss += loss.item()\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "        # generate images and metrics\n",
    "\n",
    "        parallel_size = music_embedding.shape[0]\n",
    "        unconditional_tokens = torch.zeros((1, input_embeds.shape[-2]), dtype=torch.int).cuda()\n",
    "        unconditional_tokens[0, 1:-1] = processor.pad_id\n",
    "        unconditional_embeds = model.language_model.get_input_embeddings()(unconditional_tokens)\n",
    "        gen_input_embeds = torch.zeros((input_embeds.shape[0] * 2, *input_embeds.shape[1:]),\n",
    "                                       dtype=torch.bfloat16).cuda()\n",
    "\n",
    "        for i in range(parallel_size * 2):\n",
    "            if i % 2 != 0:\n",
    "                gen_input_embeds[i] = unconditional_embeds\n",
    "            else:\n",
    "                gen_input_embeds[i] = input_embeds[i // 2]\n",
    "\n",
    "        generated_tokens = torch.zeros((parallel_size, batch_input_ids.shape[-1]), dtype=torch.int).cuda()\n",
    "        inputs_embeds = gen_input_embeds\n",
    "        for i in range(batch_input_ids.shape[-1]):\n",
    "            outputs = model.language_model.model(inputs_embeds=inputs_embeds, use_cache=True,\n",
    "                                                 past_key_values=outputs.past_key_values if i != 0 else None)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            logits = model.gen_head(hidden_states[:, -1, :])\n",
    "            logit_cond = logits[0::2, :]\n",
    "            logit_uncond = logits[1::2, :]\n",
    "\n",
    "            logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_tokens[:, i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "            next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n",
    "            img_embeds = model.prepare_gen_img_embeds(next_token)\n",
    "            inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "        dec = model.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int),\n",
    "                                                 shape=[parallel_size, 8, img_size // patch_size,\n",
    "                                                        img_size // patch_size])\n",
    "        dec = dec.to(torch.float32)\n",
    "        dec = torch.clamp((dec + 1) / 2 * 255, min=0, max=255)\n",
    "\n",
    "        visual_img = dec.to(torch.uint8)\n",
    "        target_images = batch[\"images\"].cuda()\n",
    "\n",
    "        if target_images.dtype != torch.uint8:\n",
    "            target_images = (target_images * 255.0).to(torch.uint8)\n",
    "        if target_images.shape[1] != 3:\n",
    "            target_images = target_images.permute(0, 3, 1, 2)\n",
    "\n",
    "        fid.update(target_images, real=True)\n",
    "        fid.update(visual_img, real=False)\n",
    "        inception_score.update(visual_img)\n",
    "\n",
    "    val_res = {\n",
    "        \"loss\": sumloss / num_batches if num_batches > 0 else 0,\n",
    "        \"num_batches\": num_batches,\n",
    "        # mock values\n",
    "        \"imagebind_sim\": 0,\n",
    "    }\n",
    "    try:\n",
    "        val_res[\"fid\"] = metrics[\"fid\"].compute()\n",
    "        val_res[\"inception_score_mean\"], val_res[\"inception_score_std\"] = metrics[\"inception_score\"].compute()\n",
    "    except RuntimeError as e:\n",
    "        val_res[\"fid\"] = 0\n",
    "        val_res[\"inception_score_mean\"], val_res[\"inception_score_std\"] = 0, 0\n",
    "\n",
    "    return val_res"
   ],
   "id": "3ff6f90c3583bed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = {\n",
    "        \"fid\": FrechetInceptionDistance(feature=768).cuda(),\n",
    "        \"inception_score\": InceptionScore(feature='logits_unbiased', splits=10).cuda()\n",
    "    }\n",
    "\n",
    "proj = AudioProjection(1024, 2048, scale_factor=2, sequal_len=2).cuda()\n",
    "val_dataloader = DataLoader(dataset, batch_size=1, shuffle=True,\n",
    "                            num_workers=TrainConfig.dataloader_num_workers,\n",
    "                            collate_fn=get_collate_fn(vl_gpt)\n",
    "                            )\n",
    "val_loop(vl_gpt, vl_chat_processor, proj, val_dataloader, metrics=metrics)"
   ],
   "id": "469f2e8a0bc3a724",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# get samples in fid\n",
   "id": "d62ebe4b44e897fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d1f6cb8ec2e7563",
   "metadata": {},
   "source": [
    "def train(\n",
    "        model: MultiModalityCausalLM,\n",
    "        projection: AudioProjection,\n",
    "        processor: VLChatProcessor,\n",
    "        train_dataloader: DataLoader,\n",
    "        val_dataloader: DataLoader,\n",
    "        train_config: TrainConfig,\n",
    "        device_placement=True,\n",
    "        ):\n",
    "    best_fid = 0\n",
    "\n",
    "    trainable_parameters = list(projection.parameters())\n",
    "    optimizer = Adam(trainable_parameters, lr=train_config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    accelerator = accelerate.Accelerator(device_placement=device_placement)\n",
    "    accelerator.gradient_accumulation_steps = train_config.gradient_accumulation_steps\n",
    "\n",
    "    model, projection, optimizer, train_dataloader, val_dataloader = accelerator.prepare(model, projection, optimizer,\n",
    "                                                                                         train_dataloader,\n",
    "                                                                                         val_dataloader)\n",
    "\n",
    "    metrics = {\n",
    "        \"fid\": FrechetInceptionDistance(feature=2048).to(model.device),\n",
    "        \"inception_score\": InceptionScore(feature='logits_unbiased', splits=10).to(model.device)\n",
    "    }\n",
    "\n",
    "    for epoch in range(train_config.num_epochs):\n",
    "        # train_loop(accelerator, model, projection, optimizer, train_dataloader, epoch=epoch, criterion=criterion,\n",
    "        #            train_config=train_config)\n",
    "\n",
    "        if epoch % train_config.evaluate_every_epoch_mod == 0:\n",
    "            print(\"Evaluating model for epoch: \", epoch)\n",
    "            validation_metrics = val_loop(model, processor, projection, val_dataloader, epoch=epoch, metrics=metrics,\n",
    "                                          generate_freq=1)\n",
    "\n",
    "            final_fid_score = metrics[\"fid\"].compute()\n",
    "            final_is_mean, final_is_std =  metrics[\"inception_score\"].compute()\n",
    "            validation_metrics[\"fid\"] = final_fid_score\n",
    "            validation_metrics[\"is_mean\"] = final_is_mean\n",
    "            validation_metrics[\"is_std\"] = final_is_std\n",
    "\n",
    "            print(f\"Epoch {epoch} validation metrics: {validation_metrics}\")\n",
    "            if final_fid_score < best_fid or best_fid == 0:\n",
    "                best_fid = final_fid_score\n",
    "                print(\"New best fid: \", best_fid)\n",
    "\n",
    "\n",
    "def freeze_model(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    return\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9ddd6b42c826e5c1",
   "metadata": {},
   "source": [
    "proj = AudioProjection(1024, 2048, scale_factor=2)\n",
    "train_dataloader = DataLoader(dataset, batch_size=TrainConfig.train_batch_size, shuffle=True,\n",
    "                            num_workers=TrainConfig.dataloader_num_workers,\n",
    "                            collate_fn=get_collate_fn(vl_gpt)\n",
    "                            )\n",
    "#\n",
    "# train(\n",
    "#     model=vl_gpt,\n",
    "#     projection=proj,\n",
    "#     processor=vl_chat_processor,\n",
    "#     train_dataloader=train_dataloader,\n",
    "#     val_dataloader=None,\n",
    "#     train_config=TrainConfig,\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "accelerator = accelerate.Accelerator(device_placement=True)\n",
    "trainable_parameters = list(proj.parameters())\n",
    "model, projection, optimizer, train_dataloader, val_dataloader = accelerator.prepare(vl_gpt, proj, Adam(trainable_parameters, lr=0.0001),\n",
    "                                                                                         train_dataloader,\n",
    "                                                                                         val_dataloader)\n",
    "accelerator.save_state(f\"model_epoch_{epoch}.pt\")"
   ],
   "id": "81f21bc73f8a21c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7134bb31549d058",
   "metadata": {},
   "source": [
    "# empty cuda\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d8737d61e65409f",
   "metadata": {},
   "source": [
    "ra = torch.rand(1, 3, 384, 384)\n",
    "images = torch.cat([ra], dim=0)\n",
    "images.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ConfigurableAudioProjection(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,  # Dimension of the input audio features\n",
    "                 embedding_dim: int,  # Dimension of the output embedding for each sequence element\n",
    "                 sequence_length: int,  # Desired output sequence length\n",
    "                 mlp_layers: int = 2,  # Number of layers in the MLP\n",
    "                 mlp_expansion_factor: int = 2,  # Expansion factor for hidden layers in MLP\n",
    "                 dropout: float = 0.1,\n",
    "                 activation_fn: str = 'gelu',  # 'gelu', 'relu', 'leaky_relu', 'swish'\n",
    "                 use_final_l2_norm: bool = True,  # Whether to L2 normalize the final output embeddings\n",
    "                 use_learnable_scale: bool = True,  # If True, applies a learnable scalar after potential normalization\n",
    "                 initial_scale: float = 1.0,  # Initial value for the learnable scale, or fixed scale if not learnable\n",
    "                 mlp_outputs_sequence: bool = False\n",
    "                 # If True, MLP directly outputs flattened sequence; if False, MLP outputs embedding_dim then expands\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.use_final_l2_norm = use_final_l2_norm\n",
    "        self.use_learnable_scale = use_learnable_scale\n",
    "        self.mlp_outputs_sequence = mlp_outputs_sequence\n",
    "\n",
    "        if activation_fn.lower() == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation_fn.lower() == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU(0.2)\n",
    "        elif activation_fn.lower() == 'swish' or activation_fn.lower() == 'silu':\n",
    "            self.activation = nn.SiLU()\n",
    "        else:  # Default to GELU\n",
    "            self.activation = nn.GELU()\n",
    "\n",
    "        self.projection_mlp = nn.ModuleList()\n",
    "        current_dim = input_dim\n",
    "\n",
    "        if not self.mlp_outputs_sequence:\n",
    "            for i in range(mlp_layers):\n",
    "                is_last_mlp_layer = (i == mlp_layers - 1)\n",
    "                layer_output_dim = embedding_dim if is_last_mlp_layer else embedding_dim * mlp_expansion_factor\n",
    "\n",
    "                self.projection_mlp.append(nn.Linear(current_dim, layer_output_dim))\n",
    "                self.projection_mlp.append(nn.LayerNorm(layer_output_dim))\n",
    "                if not is_last_mlp_layer:  # No activation/dropout after the final MLP layer output\n",
    "                    self.projection_mlp.append(self.activation)\n",
    "                    self.projection_mlp.append(nn.Dropout(dropout))\n",
    "                current_dim = layer_output_dim\n",
    "\n",
    "            self.sequence_expansion_layer = nn.Linear(current_dim, sequence_length * embedding_dim)\n",
    "        else:\n",
    "            # Path B: MLP directly projects the input to the full flattened sequence dimension.\n",
    "            # The last layer of the MLP will output sequence_length * embedding_dim.\n",
    "            final_flattened_dim = sequence_length * embedding_dim\n",
    "            for i in range(mlp_layers):\n",
    "                is_last_mlp_layer = (i == mlp_layers - 1)\n",
    "                # For intermediate layers, hidden_dim can be an expansion of input or final_flattened_dim\n",
    "                # For the last layer, output is final_flattened_dim\n",
    "                if not is_last_mlp_layer:\n",
    "                    # Heuristic: expand based on a factor of the larger of input or eventual flat output\n",
    "                    hidden_dim_base = max(input_dim, final_flattened_dim)\n",
    "                    layer_output_dim = int(hidden_dim_base * mlp_expansion_factor)\n",
    "                else:\n",
    "                    layer_output_dim = final_flattened_dim\n",
    "\n",
    "                self.projection_mlp.append(nn.Linear(current_dim, layer_output_dim))\n",
    "                self.projection_mlp.append(nn.LayerNorm(layer_output_dim))\n",
    "                if not is_last_mlp_layer:\n",
    "                    self.projection_mlp.append(self.activation)\n",
    "                    self.projection_mlp.append(nn.Dropout(dropout))\n",
    "                current_dim = layer_output_dim\n",
    "            self.sequence_expansion_layer = None  # MLP directly outputs the final flat shape\n",
    "\n",
    "        # Setup scaling parameter\n",
    "        if self.use_learnable_scale:\n",
    "            self.scale_param = nn.Parameter(torch.tensor(float(initial_scale)))\n",
    "        else:\n",
    "            # If not learnable, register as a buffer if it's not 1.0 (to avoid unnecessary multiplication by 1.0)\n",
    "            if float(initial_scale) != 1.0:\n",
    "                self.register_buffer('scale_param', torch.tensor(float(initial_scale)))\n",
    "            else:\n",
    "                self.scale_param = 1.0  # Will be a float, not a tensor or parameter\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is expected to be [B, input_dim]\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Pass through MLP\n",
    "        for layer in self.projection_mlp:\n",
    "            x = layer(x)\n",
    "\n",
    "        # If MLP didn't output the sequence directly, use the expansion layer\n",
    "        if not self.mlp_outputs_sequence and self.sequence_expansion_layer is not None:\n",
    "            x = self.sequence_expansion_layer(x)\n",
    "            # x is now [B, sequence_length * embedding_dim]\n",
    "\n",
    "        # Reshape to [B, sequence_length, embedding_dim]\n",
    "        try:\n",
    "            x = x.view(B, self.sequence_length, self.embedding_dim)\n",
    "        except RuntimeError as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to reshape tensor of shape {x.shape} to ({B}, {self.sequence_length}, {self.embedding_dim}). Original error: {e}\")\n",
    "\n",
    "        if self.use_final_l2_norm:\n",
    "            x = F.normalize(x, p=2, dim=-1)\n",
    "\n",
    "        # Apply scaling (learnable or fixed)\n",
    "        # If self.scale_param is 1.0 (float), this multiplication is trivial but harmless.\n",
    "        x = x * self.scale_param\n",
    "\n",
    "        return x"
   ],
   "id": "864c79cb528ffc06",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
