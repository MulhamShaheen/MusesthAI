{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:16:53.357663Z",
     "start_time": "2025-04-30T10:16:53.344717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add janus to os path\n",
    "import sys\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "janus_path = os.path.abspath(\"../Janus/janus\")\n",
    "sys.path.append(janus_path)\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ],
   "id": "ee7f503228a27a79",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-30T10:17:04.123599Z",
     "start_time": "2025-04-30T10:16:53.414014Z"
    }
   },
   "source": [
    "from abc import ABC\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from imagebind.models.imagebind_model import ImageBindModel\n",
    "from imagebind import data\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.models.processing_vlm import VLChatProcessorOutput\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mulham/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/mulham/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "/home/mulham/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mulham/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:602: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Audio Projection",
   "id": "527490705dcc9d36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T19:16:11.140550Z",
     "start_time": "2025-04-29T19:16:11.135771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AudioProjection(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, sequal_len=32, scale_factor=2):\n",
    "        super(AudioProjection, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.sequal_len = sequal_len\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(input_dim, scale_factor * output_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(scale_factor * output_dim, sequal_len * output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.fc1(x)  # → [B, scale_factor * output_dim]\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)  # → [B, sequal_len * output_dim]\n",
    "        x = torch.reshape(x, (B, self.sequal_len, self.output_dim))\n",
    "\n",
    "        return x"
   ],
   "id": "dcd1a4761cc629bb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T19:16:13.468215Z",
     "start_time": "2025-04-29T19:16:11.222131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(2, 1024)\n",
    "with torch.no_grad():\n",
    "    proj = AudioProjection(1024, 2048, scale_factor=2)\n",
    "    res = proj(x)\n",
    "print(res.shape)"
   ],
   "id": "1e33a8e27ec7cf53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 2048])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Janus Image Generator",
   "id": "bc136b2c8731651"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "AVAILABLE_MODELS = [\"Janus-1.3B\", \"Janus-Pro-1B\", \"JanusFlow-1.3B\"]\n",
    "\n",
    "\n",
    "class JanusImageGenerator:\n",
    "    name = \"Janus Image Generator\"\n",
    "\n",
    "    @classmethod\n",
    "    def init_model(cls, config):\n",
    "        model_name = config.get(\"model_name\", \"Janus-Pro-1B\")\n",
    "        cls.sys_prompt = config.get(\"sys_prompt\", \"Abstract art for representing emotions\")\n",
    "        if model_name not in AVAILABLE_MODELS:\n",
    "            logging.warning(f\"Model {model_name} not available. Using {AVAILABLE_MODELS[0]} instead.\")\n",
    "            model_name = AVAILABLE_MODELS[0]\n",
    "\n",
    "        model_path = f\"deepseek-ai/{model_name}\"\n",
    "        cls.vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "        cls.tokenizer = cls.vl_chat_processor.tokenizer\n",
    "\n",
    "        cls.vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, trust_remote_code=True\n",
    "        )\n",
    "        cls.vl_gpt = cls.vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "        cls.model = cls.vl_gpt\n",
    "        cls.audio_embeds_shape = {0: 2, 2: 2048}\n",
    "        cls.audio_embeds_type = torch.bfloat16\n",
    "        # hardcoded need to learn more about this\n",
    "        cls.parallel_size = 1\n",
    "        cls.img_size = 384\n",
    "        cls.patch_size = 16\n",
    "\n",
    "    @classmethod\n",
    "    def _preprocess_input(cls, inputs):\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"User\",\n",
    "                \"content\": inputs,\n",
    "            },\n",
    "            {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "        ]\n",
    "\n",
    "        sft_format = cls.vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "            conversations=conversation,\n",
    "            sft_format=cls.vl_chat_processor.sft_format,\n",
    "            system_prompt=\"\",\n",
    "        )\n",
    "\n",
    "        return sft_format + cls.vl_chat_processor.image_start_tag\n",
    "\n",
    "    @classmethod\n",
    "    def _postprocess_output(cls, outputs):\n",
    "        # concated = torch.cat([outputs, torch.zeros_like(outputs)], dim=1)\n",
    "        outputs = outputs.numpy()\n",
    "        outputs = np.clip((outputs + 1) / 2 * 255, 0, 255)\n",
    "        image = Image.fromarray(outputs.astype(np.uint8))\n",
    "\n",
    "        return image\n",
    "\n",
    "    @classmethod\n",
    "    def invoke_model(cls, prompt: str,\n",
    "                     temperature: float = 1,\n",
    "                     cfg_weight: float = 5,\n",
    "                     image_token_num_per_image: int = 576,\n",
    "                     audio_embeds: torch.Tensor = None,\n",
    "                     **kwargs):\n",
    "\n",
    "        input_ids = cls.vl_chat_processor.tokenizer.encode(prompt)\n",
    "        input_ids = torch.LongTensor(input_ids)\n",
    "\n",
    "        tokens = torch.zeros((cls.parallel_size * 2, len(input_ids)), dtype=torch.int).cuda()\n",
    "        for i in range(cls.parallel_size * 2):\n",
    "            tokens[i, :] = input_ids\n",
    "            if i % 2 != 0:\n",
    "                tokens[i, 1:-1] = cls.vl_chat_processor.pad_id\n",
    "        cls.vl_gpt.language_model.config._attn_implementation = 'eager'\n",
    "\n",
    "        inputs_embeds = cls.vl_gpt.language_model.get_input_embeddings()(tokens)\n",
    "        print(inputs_embeds.shape)\n",
    "        if audio_embeds is not None:\n",
    "            inputs_embeds = torch.cat([audio_embeds, inputs_embeds], dim=1)\n",
    "\n",
    "        generated_tokens = torch.zeros((cls.parallel_size, image_token_num_per_image), dtype=torch.int).cuda()\n",
    "\n",
    "        for i in range(image_token_num_per_image):\n",
    "            outputs = cls.vl_gpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True,\n",
    "                                                      past_key_values=outputs.past_key_values if i != 0 else None)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "\n",
    "            logits = cls.vl_gpt.gen_head(hidden_states[:, -1, :])\n",
    "            logit_cond = logits[0::2, :]\n",
    "            logit_uncond = logits[1::2, :]\n",
    "\n",
    "            logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_tokens[:, i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "            next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n",
    "            img_embeds = cls.vl_gpt.prepare_gen_img_embeds(next_token)\n",
    "            inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "        dec: torch.Tensor = cls.vl_gpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int),\n",
    "                                                                    shape=[cls.parallel_size, 8,\n",
    "                                                                           cls.img_size // cls.patch_size,\n",
    "                                                                           cls.img_size // cls.patch_size])\n",
    "        dec = dec.to(torch.float32).cpu().permute(0, 2, 3, 1)[0]\n",
    "\n",
    "        return dec\n",
    "\n",
    "    @classmethod\n",
    "    def generate(cls, inputs: str) -> Image:\n",
    "        inputs = cls._preprocess_input(inputs)\n",
    "        outputs = cls.invoke_model(inputs=inputs)\n",
    "        return cls._postprocess_output(outputs)\n",
    "\n",
    "    @classmethod\n",
    "    def generate_from_embeds(cls, inputs: np.ndarray) -> Image:\n",
    "        prompt = cls._preprocess_input(cls.sys_prompt)\n",
    "        input_tensor = torch.from_numpy(inputs).to(cls.audio_embeds_type).cuda()\n",
    "\n",
    "        if not all([input_tensor.shape[d] == s for d, s in cls.audio_embeds_shape.items()]):\n",
    "            logger.error(f\"Input tensor had shape {inputs.shape} was expected {cls.audio_embeds_shape}\")\n",
    "\n",
    "        output = cls.invoke_model(prompt, audio_embeds=input_tensor)\n",
    "        image = cls._postprocess_output(output)\n",
    "        return image\n"
   ],
   "id": "ec0585fc37e8aed8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training config",
   "id": "20d550bf896ee3f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:17:04.190286Z",
     "start_time": "2025-04-30T10:17:04.184541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrainConfig:  # copy from hw-multimodal-llm-solved need to be changed\n",
    "    log_level = \"DEBUG\"\n",
    "\n",
    "    # Training\n",
    "    num_epochs = 5\n",
    "    train_batch_size = 8\n",
    "    val_batch_size = 1\n",
    "    log_grad_norm = True\n",
    "    learning_rate = 1e-4\n",
    "    gradient_accumulation_steps = 1\n",
    "\n",
    "    evaluate_every_epoch_mod = 4\n",
    "    save_model_every_epoch_mod = 1\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "    # Model\n",
    "\n",
    "    # Projector\n",
    "    projector_input_dim = 1024\n",
    "\n",
    "    # Data\n",
    "    few_train_samples = None\n",
    "    few_val_samples = 100\n",
    "    dataloader_num_workers = 0\n",
    "\n",
    "    train_dataset_path = \"\"\n",
    "    audio_embeds_train_prefix = \"\"\n",
    "\n",
    "    val_dataset_path = \"\"\n",
    "    audio_embeds_val_prefix = \"\"\n"
   ],
   "id": "6adb6067b1dff347",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:17:04.273619Z",
     "start_time": "2025-04-30T10:17:04.257245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "import argparse\n",
    "\n",
    "import pathlib\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import logging\n",
    "# import evaluate\n",
    "\n",
    "# import datasets\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import wandb\n",
    "# from wandb import sdk as wandb_sdk\n",
    "\n",
    "# import accelerate\n"
   ],
   "id": "790d46bd401415af",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Pipelines research",
   "id": "ce856e12d58c7d64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Image decoding",
   "id": "d5dcea8eb5622519"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:17:06.221844Z",
     "start_time": "2025-04-30T10:17:04.343892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = f\"deepseek-ai/Janus-Pro-1B\"\n",
    "\n",
    "prompt = \"Abstract art for representing emotions\"\n",
    "\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "input_ids = torch.LongTensor(input_ids)"
   ],
   "id": "3d01ed0b91a131ff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Some kwargs in processor config are unused and will not have any effect: ignore_id, image_tag, mask_prompt, sft_format, num_image_tokens, add_special_token. \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:18:10.860269Z",
     "start_time": "2025-04-30T10:17:06.547654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True,\n",
    ")\n",
    "vl_gpt.language_model.config._attn_implementation = 'eager'\n",
    "\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()"
   ],
   "id": "107d5511ad9d9164",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Image encoding and restoring",
   "id": "62dbe99f013d713f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:15:55.086639Z",
     "start_time": "2025-04-28T12:15:54.500805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img = Image.open(\"generated_samples/img_0.jpg\").convert(\"RGB\")\n",
    "prepare = vl_chat_processor.process_one(prompt=\"<image_placeholder>\", images=[img])\n",
    "bs, n = prepare.pixel_values.unsqueeze(0).shape[0:2]\n",
    "\n",
    "images = prepare.pixel_values.to(torch.bfloat16).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    quant, _, info = vl_gpt.gen_vision_model.encode(images)  # torch.Size([1, 3, 384, 384])\n",
    "    B, C, Hq, Wq = quant.shape\n",
    "    _, _, min_encoding_indices = info\n",
    "    image_ids = min_encoding_indices.view(B, Hq * Wq)\n",
    "    gen_embeds = vl_gpt.prepare_gen_img_embeds(image_ids)  # torch.Size([1, 576, 2048])\n",
    "    logits = vl_gpt.gen_head(gen_embeds)\n",
    "\n",
    "    probs = torch.softmax(logits / 1, dim=-1)\n",
    "    # gen_image_tokens = torch.multinomial(probs[0], num_samples=1)\n",
    "    indices = logits.max(dim=-1).indices\n",
    "    gen_image_tokens = indices\n",
    "    dec = vl_gpt.gen_vision_model.decode_code(gen_image_tokens.to(dtype=torch.int), shape=[1, 8, 24, 24])\n",
    "    dec_temp = dec\n",
    "    dec = dec.to(torch.float32).detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n",
    "    visual_img = dec.astype(np.uint8)\n",
    "    Image.fromarray(visual_img[0]).save(\"test.jpg\")"
   ],
   "id": "7a6024a4740d5dd2",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (int) and bias type (c10::BFloat16) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[43], line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m indices \u001B[38;5;241m=\u001B[39m logits\u001B[38;5;241m.\u001B[39mmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mindices\n\u001B[1;32m     18\u001B[0m gen_image_tokens \u001B[38;5;241m=\u001B[39m indices\n\u001B[0;32m---> 19\u001B[0m dec \u001B[38;5;241m=\u001B[39m \u001B[43mvl_gpt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen_vision_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgen_image_tokens\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mint\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m dec_temp \u001B[38;5;241m=\u001B[39m dec\n\u001B[1;32m     21\u001B[0m dec \u001B[38;5;241m=\u001B[39m dec\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/mnt/e/AI Talent Hub/MusesthAI/Janus/janus/models/vq_model.py:501\u001B[0m, in \u001B[0;36mVQModel.decode\u001B[0;34m(self, quant)\u001B[0m\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, quant):\n\u001B[0;32m--> 501\u001B[0m     quant \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost_quant_conv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquant\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    502\u001B[0m     dec \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(quant)\n\u001B[1;32m    503\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dec\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    553\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 554\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\n\u001B[1;32m    539\u001B[0m         F\u001B[38;5;241m.\u001B[39mpad(\n\u001B[1;32m    540\u001B[0m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    547\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups,\n\u001B[1;32m    548\u001B[0m     )\n\u001B[0;32m--> 549\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    550\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\n\u001B[1;32m    551\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Input type (int) and bias type (c10::BFloat16) should be the same"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:17:24.263106Z",
     "start_time": "2025-04-28T12:17:15.415292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dec_temp = vl_gpt.gen_vision_model.decode_code(image_ids.to(dtype=torch.int), shape=[1, 8, 24, 24])\n",
    "dec_temp = dec_temp.to(torch.float32).detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "dec_temp = np.clip((dec_temp + 1) / 2 * 255, 0, 255)\n",
    "visual_img = dec_temp.astype(np.uint8)\n",
    "Image.fromarray(visual_img[0]).save(\"test.jpg\")"
   ],
   "id": "5401a3571a7c8e08",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:10:37.053282Z",
     "start_time": "2025-04-28T12:10:37.046689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "indices = logits.max(dim=-1).indices\n",
    "indices.shape"
   ],
   "id": "a65a8588692ea4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 576])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:06:21.595465Z",
     "start_time": "2025-04-28T12:06:21.574513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Image.fromarray(images[0].permute(1, 2, 0).detach().cpu().to(torch.float).numpy()).save(\"test3.jpg\")\n",
    "img_arr = images[0].permute(1, 2, 0).detach().to(torch.float).cpu().numpy()\n",
    "img_arr = np.clip((img_arr + 1) / 2 * 255, 0, 255)\n",
    "image = Image.fromarray(img_arr.astype(\"uint8\"))\n",
    "image.save(\"test3.jpg\")"
   ],
   "id": "6fcf73e724e2797f",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:45:22.722560Z",
     "start_time": "2025-04-30T10:45:22.715408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers.models.bart.modeling_bart import shift_tokens_right  # similar utility\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def shift_image_tokens(image_ids: torch.Tensor):\n",
    "\n",
    "    image_embeds_shifted = shift_tokens_right(\n",
    "        image_ids,\n",
    "        pad_token_id=vl_chat_processor.pad_id,\n",
    "        decoder_start_token_id=vl_chat_processor.image_start_id\n",
    "    )\n",
    "    return image_embeds_shifted\n",
    "\n",
    "\n",
    "def get_image_janus_embeds(imgs: List[Image.Image]):\n",
    "    prepare = vl_chat_processor.process_one(prompt=\"<image_placeholder>\", images=imgs)\n",
    "    images = rearrange(prepare.pixel_values.unsqueeze(0).to(torch.bfloat16).to(\"cuda:0\"), \"b n c h w -> (b n) c h w\")\n",
    "    quant, _, info = vl_gpt.gen_vision_model.encode(images)  # torch.Size([1, 3, 384, 384])\n",
    "    B, C, Hq, Wq = quant.shape\n",
    "    _, _, min_encoding_indices = info\n",
    "    image_ids = min_encoding_indices.view(B, Hq * Wq)\n",
    "    # gen_embeds = vl_gpt.prepare_gen_img_embeds(image_ids)\n",
    "    gen_embeds = vl_gpt.gen_embed(image_ids)\n",
    "    gen_aligned = vl_gpt.gen_aligner(gen_embeds)\n",
    "\n",
    "    return image_ids, gen_embeds, gen_aligned"
   ],
   "id": "32fdcc223ea77c48",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:40:20.632614Z",
     "start_time": "2025-04-30T10:40:20.623370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_path = \"generated_samples/img_0.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")"
   ],
   "id": "fcc59b276609a9fa",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:48:39.779954Z",
     "start_time": "2025-04-30T10:48:26.376551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "random_tensor = torch.randn(1, 1, 2048).cuda().to(torch.bfloat16)  # audio projection tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_ids, image_embeds, image_aligned = get_image_janus_embeds([image])\n",
    "    # input_tensor — prompt embeds (text), image_bind_projection, image_embeds (concat with dim=1)\n",
    "    input_tensor = torch.concat([image_aligned], dim=1)\n",
    "\n",
    "    outputs = vl_gpt.language_model.model(inputs_embeds=input_tensor, use_cache=False, past_key_values=None,\n",
    "                                          decoder_input_ids=1)\n",
    "\n",
    "    hidden_states = outputs.last_hidden_state  # torch.Size([1, 608, 2048])\n",
    "\n",
    "    logits = vl_gpt.gen_head(hidden_states)\n",
    "    print(logits.shape)\n",
    "    probs = torch.softmax(logits[:, -576:, :], dim=-1)\n",
    "    logits = logits.permute(0, 2, 1)\n",
    "    image_ids = image_ids.squeeze(-1) # torch.Size([1, 608, 2048])\n",
    "    shifted_image_ids = shift_image_tokens(image_ids) # torch.Size([1, 576])\n",
    "    # loss = cross_entropy(logits[:, :, -576:], shifted_image_ids, ignore_index=-100)"
   ],
   "id": "935d9b8216893f3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 16384])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:48:40.526287Z",
     "start_time": "2025-04-30T10:48:40.384496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# indices = logits[:, :, -576:].max(dim=-2).indices\n",
    "probs = torch.softmax(logits[:, :, -576:], dim=-1)\n",
    "indices = probs.max(dim=-2).indices\n",
    "indices.shape"
   ],
   "id": "12439ece4f89603f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 576])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:48:53.071443Z",
     "start_time": "2025-04-30T10:48:44.994548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gen_image_tokens = indices\n",
    "dec = vl_gpt.gen_vision_model.decode_code(gen_image_tokens.to(dtype=torch.int), shape=[1, 8, 24, 24])\n",
    "dec_temp = dec\n",
    "dec = dec.to(torch.float32).detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n",
    "visual_img = dec.astype(np.uint8)\n",
    "Image.fromarray(visual_img[0]).save(\"test.jpg\")"
   ],
   "id": "b436cb0c2a36e2e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f10331b132879f62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
