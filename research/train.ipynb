{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# add janus to os path\n",
    "import sys\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "janus_path = os.path.abspath(\"../Janus/janus\")\n",
    "sys.path.append(janus_path)\n"
   ],
   "id": "ee7f503228a27a79",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from abc import ABC\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from imagebind.models.imagebind_model import ImageBindModel\n",
    "from imagebind import data\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.models.processing_vlm import VLChatProcessorOutput\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Audio Projection",
   "id": "527490705dcc9d36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AudioProjection(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, sequal_len=32, scale_factor=2):\n",
    "        super(AudioProjection, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.sequal_len = sequal_len\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(input_dim, scale_factor * output_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(scale_factor * output_dim, sequal_len * output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.fc1(x)  # → [B, scale_factor * output_dim]\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)  # → [B, sequal_len * output_dim]\n",
    "        x = torch.reshape(x, (B, self.sequal_len, self.output_dim))\n",
    "\n",
    "        return x"
   ],
   "id": "dcd1a4761cc629bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T21:55:56.806358Z",
     "start_time": "2025-04-24T21:55:53.923375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(2, 1024)\n",
    "with torch.no_grad():\n",
    "    proj = AudioProjection(1024, 2048, scale_factor=2)\n",
    "    res = proj(x)\n",
    "print(res.shape)"
   ],
   "id": "1e33a8e27ec7cf53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 2048])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Janus Image Generator",
   "id": "bc136b2c8731651"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "AVAILABLE_MODELS = [\"Janus-1.3B\", \"Janus-Pro-1B\", \"JanusFlow-1.3B\"]\n",
    "\n",
    "\n",
    "class JanusImageGenerator:\n",
    "    name = \"Janus Image Generator\"\n",
    "\n",
    "    @classmethod\n",
    "    def init_model(cls, config):\n",
    "        model_name = config.get(\"model_name\", \"Janus-Pro-1B\")\n",
    "        cls.sys_prompt = config.get(\"sys_prompt\", \"Abstract art for representing emotions\")\n",
    "        if model_name not in AVAILABLE_MODELS:\n",
    "            logging.warning(f\"Model {model_name} not available. Using {AVAILABLE_MODELS[0]} instead.\")\n",
    "            model_name = AVAILABLE_MODELS[0]\n",
    "\n",
    "        model_path = f\"deepseek-ai/{model_name}\"\n",
    "        cls.vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "        cls.tokenizer = cls.vl_chat_processor.tokenizer\n",
    "\n",
    "        cls.vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, trust_remote_code=True\n",
    "        )\n",
    "        cls.vl_gpt = cls.vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "        cls.model = cls.vl_gpt\n",
    "        cls.audio_embeds_shape = {0: 2, 2: 2048}\n",
    "        cls.audio_embeds_type = torch.bfloat16\n",
    "        # hardcoded need to learn more about this\n",
    "        cls.parallel_size = 1\n",
    "        cls.img_size = 384\n",
    "        cls.patch_size = 16\n",
    "\n",
    "    @classmethod\n",
    "    def _preprocess_input(cls, inputs):\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"User\",\n",
    "                \"content\": inputs,\n",
    "            },\n",
    "            {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "        ]\n",
    "\n",
    "        sft_format = cls.vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "            conversations=conversation,\n",
    "            sft_format=cls.vl_chat_processor.sft_format,\n",
    "            system_prompt=\"\",\n",
    "        )\n",
    "\n",
    "        return sft_format + cls.vl_chat_processor.image_start_tag\n",
    "\n",
    "    @classmethod\n",
    "    def _postprocess_output(cls, outputs):\n",
    "        # concated = torch.cat([outputs, torch.zeros_like(outputs)], dim=1)\n",
    "        outputs = outputs.numpy()\n",
    "        outputs = np.clip((outputs + 1) / 2 * 255, 0, 255)\n",
    "        image = Image.fromarray(outputs.astype(np.uint8))\n",
    "\n",
    "        return image\n",
    "\n",
    "    @classmethod\n",
    "    def invoke_model(cls, prompt: str,\n",
    "                     temperature: float = 1,\n",
    "                     cfg_weight: float = 5,\n",
    "                     image_token_num_per_image: int = 576,\n",
    "                     audio_embeds: torch.Tensor = None,\n",
    "                     **kwargs):\n",
    "\n",
    "        input_ids = cls.vl_chat_processor.tokenizer.encode(prompt)\n",
    "        input_ids = torch.LongTensor(input_ids)\n",
    "\n",
    "        tokens = torch.zeros((cls.parallel_size * 2, len(input_ids)), dtype=torch.int).cuda()\n",
    "        for i in range(cls.parallel_size * 2):\n",
    "            tokens[i, :] = input_ids\n",
    "            if i % 2 != 0:\n",
    "                tokens[i, 1:-1] = cls.vl_chat_processor.pad_id\n",
    "        cls.vl_gpt.language_model.config._attn_implementation = 'eager'\n",
    "\n",
    "        inputs_embeds = cls.vl_gpt.language_model.get_input_embeddings()(tokens)\n",
    "        print(inputs_embeds.shape)\n",
    "        if audio_embeds is not None:\n",
    "            inputs_embeds = torch.cat([audio_embeds, inputs_embeds], dim=1)\n",
    "\n",
    "        generated_tokens = torch.zeros((cls.parallel_size, image_token_num_per_image), dtype=torch.int).cuda()\n",
    "\n",
    "        for i in range(image_token_num_per_image):\n",
    "            outputs = cls.vl_gpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True,\n",
    "                                                      past_key_values=outputs.past_key_values if i != 0 else None)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "\n",
    "            logits = cls.vl_gpt.gen_head(hidden_states[:, -1, :])\n",
    "            logit_cond = logits[0::2, :]\n",
    "            logit_uncond = logits[1::2, :]\n",
    "\n",
    "            logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_tokens[:, i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "            next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n",
    "            img_embeds = cls.vl_gpt.prepare_gen_img_embeds(next_token)\n",
    "            inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "        dec: torch.Tensor = cls.vl_gpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int),\n",
    "                                                                    shape=[cls.parallel_size, 8,\n",
    "                                                                           cls.img_size // cls.patch_size,\n",
    "                                                                           cls.img_size // cls.patch_size])\n",
    "        dec = dec.to(torch.float32).cpu().permute(0, 2, 3, 1)[0]\n",
    "\n",
    "        return dec\n",
    "\n",
    "    @classmethod\n",
    "    def generate(cls, inputs: str) -> Image:\n",
    "        inputs = cls._preprocess_input(inputs)\n",
    "        outputs = cls.invoke_model(inputs=inputs)\n",
    "        return cls._postprocess_output(outputs)\n",
    "\n",
    "    @classmethod\n",
    "    def generate_from_embeds(cls, inputs: np.ndarray) -> Image:\n",
    "        prompt = cls._preprocess_input(cls.sys_prompt)\n",
    "        input_tensor = torch.from_numpy(inputs).to(cls.audio_embeds_type).cuda()\n",
    "\n",
    "        if not all([input_tensor.shape[d] == s for d, s in cls.audio_embeds_shape.items()]):\n",
    "            logger.error(f\"Input tensor had shape {inputs.shape} was expected {cls.audio_embeds_shape}\")\n",
    "\n",
    "        output = cls.invoke_model(prompt, audio_embeds=input_tensor)\n",
    "        image = cls._postprocess_output(output)\n",
    "        return image\n"
   ],
   "id": "ec0585fc37e8aed8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training config",
   "id": "20d550bf896ee3f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TrainConfig:  # copy from hw-multimodal-llm-solved need to be changed\n",
    "    log_level = \"DEBUG\"\n",
    "\n",
    "    # Training\n",
    "    num_epochs = 5\n",
    "    train_batch_size = 8\n",
    "    val_batch_size = 1\n",
    "    log_grad_norm = True\n",
    "    learning_rate = 1e-4\n",
    "    gradient_accumulation_steps = 1\n",
    "\n",
    "    evaluate_every_epoch_mod = 4\n",
    "    save_model_every_epoch_mod = 1\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "    # Model\n",
    "\n",
    "    # Projector\n",
    "    projector_input_dim = 1024\n",
    "\n",
    "    # Data\n",
    "    few_train_samples = None\n",
    "    few_val_samples = 100\n",
    "    dataloader_num_workers = 0\n",
    "\n",
    "    train_dataset_path = \"\"\n",
    "    audio_embeds_train_prefix = \"\"\n",
    "\n",
    "    val_dataset_path = \"\"\n",
    "    audio_embeds_val_prefix = \"\"\n"
   ],
   "id": "6adb6067b1dff347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "import argparse\n",
    "\n",
    "import pathlib\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import logging\n",
    "# import evaluate\n",
    "\n",
    "# import datasets\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import wandb\n",
    "# from wandb import sdk as wandb_sdk\n",
    "\n",
    "# import accelerate\n"
   ],
   "id": "790d46bd401415af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "image_bind = ImageBindModel()",
   "id": "ff16d1c65c5da8ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from imagebind import ModalityType\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "def prepare_image_tokens(model: MultiModalityCausalLM, batch):\n",
    "    \"\"\"\n",
    "    Get tokens from images in batch\n",
    "    \"\"\"\n",
    "\n",
    "    vq_model = model.gen_vision_model\n",
    "    if \"images\" not in batch:\n",
    "        return\n",
    "\n",
    "    quantized, _, info = vq_model.encode(batch[\"images\"])\n",
    "    image_tokens = info[2].view(batch, -1)\n",
    "\n",
    "    batch[\"image_tokens\"] = image_tokens\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def get_audio_embeds(audio_encoder: ImageBindModel, batch):\n",
    "    \"\"\"\n",
    "    From file or call imagebind\n",
    "    \"\"\"\n",
    "    audio_embeds = batch.get(\"audio_embeds\", [])\n",
    "    if audio_embeds:\n",
    "        return audio_embeds\n",
    "\n",
    "    inputs = {\n",
    "        ModalityType.AUDIO: data.load_and_transform_audio_data(batch[\"audio_path\"], TrainConfig.device),\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = audio_encoder(inputs)\n",
    "\n",
    "    batch[\"audio_embeds\"] = embeddings\n",
    "    return batch\n",
    "\n",
    "\n",
    "def get_image_embeds(image_encoder: ImageBindModel, batch):\n",
    "    \"\"\"\n",
    "    From file or call imagebind\n",
    "    \"\"\"\n",
    "    image_embeds = batch.get(\"image_embeds\", [])\n",
    "    if image_embeds:\n",
    "        return image_embeds\n",
    "\n",
    "    inputs = {\n",
    "        ModalityType.VISION: data.load_and_transform_vision_data(batch[\"image_path\"], TrainConfig.device),\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = image_encoder(inputs)\n",
    "\n",
    "    batch[\"image_embeds\"] = embeddings\n",
    "    return batch\n",
    "\n",
    "\n",
    "def save_model(train_config: TrainConfig, model: AudioProjection, path: pathlib.Path):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"save model to {path}\")\n",
    "\n",
    "    model.save_pretrained(path)\n",
    "    if train_config.llm_train_lora:\n",
    "        model.lm_model.save_pretrained(path.joinpath(\"lora_adapter\"))\n",
    "\n",
    "    return\n",
    "\n"
   ],
   "id": "e23b3950bdc65d22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Pipelines research",
   "id": "ce856e12d58c7d64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# inputs_embeds — prompt embeds (text), image_bind_projection, image_embeds (concat with dim=1)\n",
    "model_path = f\"deepseek-ai/Janus-Pro-1B\"\n",
    "\n",
    "prompt = \"Abstract art for representing emotions\"\n",
    "\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "input_ids = torch.LongTensor(input_ids)"
   ],
   "id": "3d01ed0b91a131ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T21:57:38.736934Z",
     "start_time": "2025-04-24T21:57:14.163942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True,\n",
    ")\n",
    "vl_gpt.language_model.config._attn_implementation = 'eager'\n",
    "\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()"
   ],
   "id": "107d5511ad9d9164",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T21:57:49.072886Z",
     "start_time": "2025-04-24T21:57:48.962247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers.models.bart.modeling_bart import shift_tokens_right  # similar utility\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def get_image_janus_embeds(imgs: List[Image.Image]):\n",
    "    prepare = vl_chat_processor.process_one(prompt=\"<image_placeholder>\", images=imgs)\n",
    "    bs, n = prepare.pixel_values.unsqueeze(0).shape[0:2]\n",
    "    images = rearrange(prepare.pixel_values.unsqueeze(0).to(torch.bfloat16).to(\"cuda:0\"), \"b n c h w -> (b n) c h w\")\n",
    "    image_embeds_shifted = shift_tokens_right(\n",
    "        images,\n",
    "        pad_token_id=vl_chat_processor.pad_id,\n",
    "        decoder_start_token_id=vl_chat_processor.image_start_id\n",
    "    )\n",
    "    images_embeds = vl_gpt.aligner(vl_gpt.vision_model(image_embeds_shifted))\n",
    "\n",
    "    return images_embeds"
   ],
   "id": "32fdcc223ea77c48",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T21:57:53.844518Z",
     "start_time": "2025-04-24T21:57:51.787276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_path = \"generated_samples/img_0.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "image_embeds = get_image_janus_embeds([image])\n",
    "image_embeds.shape"
   ],
   "id": "fcc59b276609a9fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 576, 2048])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:00:43.352643Z",
     "start_time": "2025-04-24T22:00:01.510298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "random_tensor = torch.randn(1, 576, 2048).cuda().to(torch.bfloat16)  # audio projection tensor\n",
    "image_tensor = get_image_janus_embeds([image])\n",
    "input_tensor = torch.concat([random_tensor, image_tensor], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = vl_gpt.language_model.model(inputs_embeds=input_tensor, use_cache=False, past_key_values=None,\n",
    "                                          decoder_input_ids=1)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    print(hidden_states.shape)\n",
    "\n",
    "    logits = vl_gpt.gen_head(hidden_states)\n",
    "    labels_logits = logits[:, -576:, :]\n",
    "    pred_logits = logits[:, :-576, :]\n",
    "\n",
    "    labels_probs = torch.softmax(labels_logits, dim=-1)\n",
    "    pred_probs = torch.softmax(pred_logits, dim=-1)\n",
    "    print(pred_probs.shape)\n",
    "    pred_probs = pred_probs.permute(0, 2, 1)\n",
    "    pred_logits = pred_logits.permute(0, 2, 1)\n",
    "\n",
    "    labels_tokens = torch.multinomial(labels_probs[0], num_samples=1)\n",
    "    labels_tokens = labels_tokens.squeeze(-1).unsqueeze(0)\n",
    "    print(labels_tokens.shape)\n",
    "    loss = cross_entropy(pred_logits, labels_tokens, ignore_index=-100)"
   ],
   "id": "935d9b8216893f3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152, 2048])\n",
      "torch.Size([1, 576, 16384])\n",
      "torch.Size([1, 576])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:00:44.034330Z",
     "start_time": "2025-04-24T22:00:44.026420Z"
    }
   },
   "cell_type": "code",
   "source": "loss.item()",
   "id": "b436cb0c2a36e2e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.375"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
