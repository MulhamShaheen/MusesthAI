{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:56:41.729555Z",
     "start_time": "2025-04-23T21:56:41.724264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add janus to os path\n",
    "import sys\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "janus_path = os.path.abspath(\"../Janus/janus\")\n",
    "sys.path.append(janus_path)\n"
   ],
   "id": "ee7f503228a27a79",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T21:56:51.976105Z",
     "start_time": "2025-04-23T21:56:41.964454Z"
    }
   },
   "source": [
    "from abc import ABC\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from imagebind.models.imagebind_model import ImageBindModel\n",
    "from imagebind import data\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.models.processing_vlm import VLChatProcessorOutput\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mulham/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/mulham/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "/home/mulham/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mulham/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:602: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Audio Projection",
   "id": "527490705dcc9d36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:56:51.988193Z",
     "start_time": "2025-04-23T21:56:51.982598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AudioProjection(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, sequal_len=32, scale_factor=2):\n",
    "        super(AudioProjection, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.sequal_len = sequal_len\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(input_dim, scale_factor * output_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(scale_factor * output_dim, sequal_len * output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.fc1(x)  # → [B, scale_factor * output_dim]\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)  # → [B, sequal_len * output_dim]\n",
    "        x = torch.reshape(x, (B, self.sequal_len, self.output_dim))\n",
    "\n",
    "        return x"
   ],
   "id": "dcd1a4761cc629bb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:46:19.463159Z",
     "start_time": "2025-04-23T21:46:18.949644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(2, 1024)\n",
    "with torch.no_grad():\n",
    "    proj = AudioProjection(1024, 2048, scale_factor=2)\n",
    "    res = proj(x)\n",
    "print(res.shape)"
   ],
   "id": "1e33a8e27ec7cf53",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AudioProjection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1024\u001B[39m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m----> 3\u001B[0m     proj \u001B[38;5;241m=\u001B[39m \u001B[43mAudioProjection\u001B[49m(\u001B[38;5;241m1024\u001B[39m, \u001B[38;5;241m2048\u001B[39m, scale_factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m      4\u001B[0m     res \u001B[38;5;241m=\u001B[39m proj(x)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(res\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'AudioProjection' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Janus Image Generator",
   "id": "bc136b2c8731651"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T17:50:10.301771Z",
     "start_time": "2025-04-21T17:50:10.269812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "AVAILABLE_MODELS = [\"Janus-1.3B\", \"Janus-Pro-1B\", \"JanusFlow-1.3B\"]\n",
    "\n",
    "\n",
    "class JanusImageGenerator:\n",
    "    name = \"Janus Image Generator\"\n",
    "\n",
    "    @classmethod\n",
    "    def init_model(cls, config):\n",
    "        model_name = config.get(\"model_name\", \"Janus-Pro-1B\")\n",
    "        cls.sys_prompt = config.get(\"sys_prompt\", \"Abstract art for representing emotions\")\n",
    "        if model_name not in AVAILABLE_MODELS:\n",
    "            logging.warning(f\"Model {model_name} not available. Using {AVAILABLE_MODELS[0]} instead.\")\n",
    "            model_name = AVAILABLE_MODELS[0]\n",
    "\n",
    "        model_path = f\"deepseek-ai/{model_name}\"\n",
    "        cls.vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "        cls.tokenizer = cls.vl_chat_processor.tokenizer\n",
    "\n",
    "        cls.vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, trust_remote_code=True\n",
    "        )\n",
    "        cls.vl_gpt = cls.vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "        cls.model = cls.vl_gpt\n",
    "        cls.audio_embeds_shape = {0: 2, 2: 2048}\n",
    "        cls.audio_embeds_type = torch.bfloat16\n",
    "        # hardcoded need to learn more about this\n",
    "        cls.parallel_size = 1\n",
    "        cls.img_size = 384\n",
    "        cls.patch_size = 16\n",
    "\n",
    "    @classmethod\n",
    "    def _preprocess_input(cls, inputs):\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"User\",\n",
    "                \"content\": inputs,\n",
    "            },\n",
    "            {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "        ]\n",
    "\n",
    "        sft_format = cls.vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "            conversations=conversation,\n",
    "            sft_format=cls.vl_chat_processor.sft_format,\n",
    "            system_prompt=\"\",\n",
    "        )\n",
    "\n",
    "        return sft_format + cls.vl_chat_processor.image_start_tag\n",
    "\n",
    "    @classmethod\n",
    "    def _postprocess_output(cls, outputs):\n",
    "        # concated = torch.cat([outputs, torch.zeros_like(outputs)], dim=1)\n",
    "        outputs = outputs.numpy()\n",
    "        outputs = np.clip((outputs + 1) / 2 * 255, 0, 255)\n",
    "        image = Image.fromarray(outputs.astype(np.uint8))\n",
    "\n",
    "        return image\n",
    "\n",
    "    @classmethod\n",
    "    def invoke_model(cls, prompt: str,\n",
    "                     temperature: float = 1,\n",
    "                     cfg_weight: float = 5,\n",
    "                     image_token_num_per_image: int = 576,\n",
    "                     audio_embeds: torch.Tensor = None,\n",
    "                     **kwargs):\n",
    "\n",
    "        input_ids = cls.vl_chat_processor.tokenizer.encode(prompt)\n",
    "        input_ids = torch.LongTensor(input_ids)\n",
    "\n",
    "        tokens = torch.zeros((cls.parallel_size * 2, len(input_ids)), dtype=torch.int).cuda()\n",
    "        for i in range(cls.parallel_size * 2):\n",
    "            tokens[i, :] = input_ids\n",
    "            if i % 2 != 0:\n",
    "                tokens[i, 1:-1] = cls.vl_chat_processor.pad_id\n",
    "        cls.vl_gpt.language_model.config._attn_implementation = 'eager'\n",
    "\n",
    "        inputs_embeds = cls.vl_gpt.language_model.get_input_embeddings()(tokens)\n",
    "        print(inputs_embeds.shape)\n",
    "        if audio_embeds is not None:\n",
    "            inputs_embeds = torch.cat([audio_embeds, inputs_embeds], dim=1)\n",
    "\n",
    "        generated_tokens = torch.zeros((cls.parallel_size, image_token_num_per_image), dtype=torch.int).cuda()\n",
    "\n",
    "        for i in range(image_token_num_per_image):\n",
    "            outputs = cls.vl_gpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True,\n",
    "                                                      past_key_values=outputs.past_key_values if i != 0 else None)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "\n",
    "            logits = cls.vl_gpt.gen_head(hidden_states[:, -1, :])\n",
    "            logit_cond = logits[0::2, :]\n",
    "            logit_uncond = logits[1::2, :]\n",
    "\n",
    "            logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_tokens[:, i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "            next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n",
    "            img_embeds = cls.vl_gpt.prepare_gen_img_embeds(next_token)\n",
    "            inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "        dec: torch.Tensor = cls.vl_gpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int),\n",
    "                                                                    shape=[cls.parallel_size, 8,\n",
    "                                                                           cls.img_size // cls.patch_size,\n",
    "                                                                           cls.img_size // cls.patch_size])\n",
    "        dec = dec.to(torch.float32).cpu().permute(0, 2, 3, 1)[0]\n",
    "\n",
    "        return dec\n",
    "\n",
    "    @classmethod\n",
    "    def generate(cls, inputs: str) -> Image:\n",
    "        inputs = cls._preprocess_input(inputs)\n",
    "        outputs = cls.invoke_model(inputs=inputs)\n",
    "        return cls._postprocess_output(outputs)\n",
    "\n",
    "    @classmethod\n",
    "    def generate_from_embeds(cls, inputs: np.ndarray) -> Image:\n",
    "        prompt = cls._preprocess_input(cls.sys_prompt)\n",
    "        input_tensor = torch.from_numpy(inputs).to(cls.audio_embeds_type).cuda()\n",
    "\n",
    "        if not all([input_tensor.shape[d] == s for d, s in cls.audio_embeds_shape.items()]):\n",
    "            logger.error(f\"Input tensor had shape {inputs.shape} was expected {cls.audio_embeds_shape}\")\n",
    "\n",
    "        output = cls.invoke_model(prompt, audio_embeds=input_tensor)\n",
    "        image = cls._postprocess_output(output)\n",
    "        return image\n"
   ],
   "id": "ec0585fc37e8aed8",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training config",
   "id": "20d550bf896ee3f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:46:21.173462Z",
     "start_time": "2025-04-23T21:46:21.168312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrainConfig:  # copy from hw-multimodal-llm-solved need to be changed\n",
    "    log_level = \"DEBUG\"\n",
    "\n",
    "    # Training\n",
    "    num_epochs = 5\n",
    "    train_batch_size = 8\n",
    "    val_batch_size = 1\n",
    "    log_grad_norm = True\n",
    "    learning_rate = 1e-4\n",
    "    gradient_accumulation_steps = 1\n",
    "\n",
    "    evaluate_every_epoch_mod = 4\n",
    "    save_model_every_epoch_mod = 1\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "    # Model\n",
    "\n",
    "    # Projector\n",
    "    projector_input_dim = 1024\n",
    "\n",
    "    # Data\n",
    "    few_train_samples = None\n",
    "    few_val_samples = 100\n",
    "    dataloader_num_workers = 0\n",
    "\n",
    "    train_dataset_path = \"\"\n",
    "    audio_embeds_train_prefix = \"\"\n",
    "\n",
    "    val_dataset_path = \"\"\n",
    "    audio_embeds_val_prefix = \"\"\n"
   ],
   "id": "6adb6067b1dff347",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:56:52.087789Z",
     "start_time": "2025-04-23T21:56:52.082761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "import argparse\n",
    "\n",
    "import pathlib\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import logging\n",
    "# import evaluate\n",
    "\n",
    "# import datasets\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import wandb\n",
    "# from wandb import sdk as wandb_sdk\n",
    "\n",
    "# import accelerate\n"
   ],
   "id": "790d46bd401415af",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "image_bind = ImageBindModel()",
   "id": "ff16d1c65c5da8ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T22:54:36.045045Z",
     "start_time": "2025-04-16T22:54:36.008799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imagebind import ModalityType\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "def prepare_image_tokens(model: MultiModalityCausalLM, batch):\n",
    "    \"\"\"\n",
    "    Get tokens from images in batch\n",
    "    \"\"\"\n",
    "\n",
    "    vq_model = model.gen_vision_model\n",
    "    if \"images\" not in batch:\n",
    "        return\n",
    "\n",
    "    quantized, _, info = vq_model.encode(batch[\"images\"])\n",
    "    image_tokens = info[2].view(batch, -1)\n",
    "\n",
    "    batch[\"image_tokens\"] = image_tokens\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def get_audio_embeds(audio_encoder: ImageBindModel, batch):\n",
    "    \"\"\"\n",
    "    From file or call imagebind\n",
    "    \"\"\"\n",
    "    audio_embeds = batch.get(\"audio_embeds\", [])\n",
    "    if audio_embeds:\n",
    "        return audio_embeds\n",
    "\n",
    "    inputs = {\n",
    "        ModalityType.AUDIO: data.load_and_transform_audio_data(batch[\"audio_path\"], TrainConfig.device),\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = audio_encoder(inputs)\n",
    "\n",
    "    batch[\"audio_embeds\"] = embeddings\n",
    "    return batch\n",
    "\n",
    "\n",
    "def get_image_embeds(image_encoder: ImageBindModel, batch):\n",
    "    \"\"\"\n",
    "    From file or call imagebind\n",
    "    \"\"\"\n",
    "    image_embeds = batch.get(\"image_embeds\", [])\n",
    "    if image_embeds:\n",
    "        return image_embeds\n",
    "\n",
    "    inputs = {\n",
    "        ModalityType.VISION: data.load_and_transform_vision_data(batch[\"image_path\"], TrainConfig.device),\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = image_encoder(inputs)\n",
    "\n",
    "    batch[\"image_embeds\"] = embeddings\n",
    "    return batch\n",
    "\n",
    "\n",
    "def save_model(train_config: TrainConfig, model: AudioProjection, path: pathlib.Path):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"save model to {path}\")\n",
    "\n",
    "    model.save_pretrained(path)\n",
    "    if train_config.llm_train_lora:\n",
    "        model.lm_model.save_pretrained(path.joinpath(\"lora_adapter\"))\n",
    "\n",
    "    return\n",
    "\n"
   ],
   "id": "e23b3950bdc65d22",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (527465804.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[6], line 14\u001B[0;36m\u001B[0m\n\u001B[0;31m    quantized, _, info = vq_model.encode(batch[])\u001B[0m\n\u001B[0m                                         ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_loop(accelerator: accelerate.Accelerator, model: AudioProjection, optimizer, train_dataloader: DataLoader,\n",
    "               epoch, criterion, last_validation_bleu=0.0, train_config=None):\n",
    "    model.train()\n",
    "    sumloss = 0\n",
    "    progress_bar = tqdm(range(len(train_dataloader)), desc=f'Epoch {epoch}')\n",
    "    # janus eval\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        with accelerator.accumulate(model):\n",
    "            audio_embeds = get_audio_embeds(image_bind, batch[\"audio_path\"])\n",
    "            image_tokens = prepare_image_tokens(model, batch)\n",
    "            proj_embeds = model(audio_embeds)\n",
    "            with torch.no_grad():\n",
    "                tokens = JanusImageGenerator.generate_from_embeds(proj_embeds)\n",
    "\n",
    "            loss = criterion(tokens, image_tokens)\n",
    "            sumloss += loss.item()\n",
    "            model.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n"
   ],
   "id": "9eee81a04f7b4630"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Pipelines research",
   "id": "ce856e12d58c7d64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:57:01.403268Z",
     "start_time": "2025-04-23T21:56:59.178283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# inputs_embeds — prompt embeds (text), image_bind_projection, image_embeds (concat with dim=1)\n",
    "model_path = f\"deepseek-ai/Janus-Pro-1B\"\n",
    "\n",
    "prompt = \"Abstract art for representing emotions\"\n",
    "\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "input_ids = torch.LongTensor(input_ids)"
   ],
   "id": "3d01ed0b91a131ff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Some kwargs in processor config are unused and will not have any effect: sft_format, image_tag, mask_prompt, add_special_token, ignore_id, num_image_tokens. \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:57:53.933018Z",
     "start_time": "2025-04-23T21:57:01.414304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True,\n",
    ")\n",
    "vl_gpt.language_model.config._attn_implementation = 'eager'\n",
    "\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()"
   ],
   "id": "107d5511ad9d9164",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:00:45.685269Z",
     "start_time": "2025-04-23T22:00:45.677967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers.models.bart.modeling_bart import shift_tokens_right  # similar utility\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def get_image_janus_embeds(imgs: List[Image.Image]):\n",
    "    prepare = vl_chat_processor.process_one(prompt=\"<image_placeholder>\", images=imgs)\n",
    "    bs, n = prepare.pixel_values.unsqueeze(0).shape[0:2]\n",
    "    images = rearrange(prepare.pixel_values.unsqueeze(0).to(torch.bfloat16).to(\"cuda:0\"), \"b n c h w -> (b n) c h w\")\n",
    "    image_embeds_shifted = shift_tokens_right(\n",
    "        images,\n",
    "        pad_token_id=vl_chat_processor.pad_id,\n",
    "        decoder_start_token_id=vl_chat_processor.image_start_id\n",
    "    )\n",
    "    images_embeds = vl_gpt.aligner(vl_gpt.vision_model(image_embeds_shifted))\n",
    "\n",
    "    return images_embeds"
   ],
   "id": "32fdcc223ea77c48",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:00:48.213230Z",
     "start_time": "2025-04-23T22:00:47.907900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_path = \"generated_samples/img_0.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "image_embeds = get_image_janus_embeds([image])\n",
    "image_embeds.shape"
   ],
   "id": "fcc59b276609a9fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 576, 2048])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:10:20.566919Z",
     "start_time": "2025-04-23T22:09:35.980028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn.functional import mse_loss, cross_entropy\n",
    "\n",
    "random_tensor = torch.randn(1, 576, 2048).cuda().to(torch.bfloat16)\n",
    "image_tensor = image_embeds = get_image_janus_embeds([image])\n",
    "random_tensor = torch.concat([random_tensor, image_tensor], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = vl_gpt.language_model.model(inputs_embeds=random_tensor, use_cache=False, past_key_values=None,\n",
    "                                          decoder_input_ids=1)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    print(hidden_states.shape)\n",
    "    print(hidden_states.dtype)\n",
    "\n",
    "    logits = vl_gpt.gen_head(hidden_states)\n",
    "    labels_logits = logits[:, -576:, :]\n",
    "    pred_logits = logits[:, :-576, :]\n",
    "\n",
    "    labels_probs = torch.softmax(labels_logits, dim=-1)\n",
    "    pred_probs = torch.softmax(pred_logits, dim=-1)\n",
    "    print(pred_probs.shape)\n",
    "    pred_probs = pred_probs.permute(0, 2, 1)\n",
    "    pred_logits = pred_logits.permute(0, 2, 1)\n",
    "    print(pred_probs.shape)\n",
    "    # pred_tokens = torch.multinomial(pred_probs[0], num_samples=256)\n",
    "    labels_tokens = torch.multinomial(labels_probs[0], num_samples=1)\n",
    "    labels_tokens = labels_tokens.squeeze(-1).unsqueeze(0)\n",
    "    print(labels_tokens.shape)\n",
    "    loss = cross_entropy(pred_logits, labels_tokens, ignore_index=-100)"
   ],
   "id": "935d9b8216893f3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1152, 2048])\n",
      "torch.bfloat16\n",
      "torch.Size([1, 576, 16384])\n",
      "torch.Size([1, 16384, 576])\n",
      "torch.Size([1, 576])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:10:20.581643Z",
     "start_time": "2025-04-23T22:10:20.575340Z"
    }
   },
   "cell_type": "code",
   "source": "loss.item()  # 9.6875",
   "id": "b436cb0c2a36e2e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.75"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:46:18.373955Z",
     "start_time": "2025-04-23T18:46:18.367954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reshape pred_probs from torch.Size([1, 256, 16384]) to torch.Size([1, 16384, 256])\n",
    "pred_probs = pred_probs.permute(0, 2, 1)\n",
    "print(pred_probs.shape)  # Should print torch.Size([1, 16384, 256])"
   ],
   "id": "8369a27a5dacf269",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16384, 256])\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T20:42:50.124379Z",
     "start_time": "2025-04-22T20:42:45.718360Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 623, 2048])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58,
   "source": [
    "from janus.utils.io import load_pil_images\n",
    "\n",
    "image_path = \"generated_samples/img_0.jpg\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"User\",\n",
    "        \"content\": \"<image_placeholder>\",\n",
    "        \"images\": [image_path],\n",
    "    },\n",
    "    {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "]\n",
    "pil_images = load_pil_images(conversation)\n",
    "prepare_inputs = vl_chat_processor(\n",
    "    conversations=conversation, images=pil_images, force_batchify=True\n",
    ").to(vl_gpt.device)\n",
    "inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "inputs_embeds.shape"
   ],
   "id": "cec12d5dc46f7ae8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T21:17:41.700424Z",
     "start_time": "2025-04-22T21:17:41.693148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_inputs_embeds(\n",
    "        input_ids: torch.LongTensor,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        images_seq_mask: torch.LongTensor,\n",
    "        images_emb_mask: torch.LongTensor,\n",
    "        **kwargs,\n",
    "):\n",
    "    bs, n = pixel_values.shape[0:2]\n",
    "    images = rearrange(pixel_values, \"b n c h w -> (b n) c h w\")\n",
    "    # [b x n, T2, D]\n",
    "    images_embeds = vl_gpt.aligner(vl_gpt.vision_model(images))\n",
    "\n",
    "    # [b x n, T2, D] -> [b, n x T2, D]\n",
    "    images_embeds = rearrange(images_embeds, \"(b n) t d -> b (n t) d\", b=bs, n=n)\n",
    "    # [b, n, T2] -> [b, n x T2]\n",
    "    images_emb_mask = rearrange(images_emb_mask, \"b n t -> b (n t)\")\n",
    "    print(images_seq_mask.shape)\n",
    "    print(images_emb_mask.shape)\n",
    "    # [b, T, D]\n",
    "    input_ids[input_ids < 0] = 0  # ignore the image embeddings\n",
    "    inputs_embeds = vl_gpt.language_model.get_input_embeddings()(input_ids)\n",
    "\n",
    "    # replace with the image embeddings\n",
    "    inputs_embeds[images_seq_mask] = images_embeds[images_emb_mask]\n",
    "    print(images_embeds.shape)\n",
    "    return inputs_embeds"
   ],
   "id": "a614210e82e739f7",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T21:17:48.841702Z",
     "start_time": "2025-04-22T21:17:44.746241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs_embeds = prepare_inputs_embeds(**prepare_inputs)\n",
    "inputs_embeds.shape"
   ],
   "id": "c6717623b69bab83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 623])\n",
      "torch.Size([1, 576])\n",
      "torch.Size([1, 576, 2048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 623, 2048])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T18:50:43.931467Z",
     "start_time": "2025-04-22T18:50:43.923318Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1])\n",
      "tensor([[13255],\n",
      "        [12526],\n",
      "        [ 2021],\n",
      "        [ 1168],\n",
      "        [ 5789],\n",
      "        [15969]])\n"
     ]
    }
   ],
   "execution_count": 31,
   "source": [
    "probs.shape\n",
    "next = torch.multinomial(probs[0], num_samples=1)\n",
    "print(next.shape)\n",
    "print(next.detach().cpu())"
   ],
   "id": "5105ba3fa422272b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:47:27.931400Z",
     "start_time": "2025-04-22T19:47:26.695585Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m a \u001B[38;5;241m=\u001B[39m vl_gpt\u001B[38;5;241m.\u001B[39mprepare_gen_img_embeds(\u001B[38;5;28mnext\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43ma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Tensor' object has no attribute 'loss'"
     ]
    }
   ],
   "execution_count": 35,
   "source": [
    "a = vl_gpt.prepare_gen_img_embeds(next)\n",
    "a.loss"
   ],
   "id": "96c5aeab1b18f1ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vision_model = vl_gpt.gen_vision_model\n",
    "with torch.no_grad():\n",
    "    quantized, a, info = vision_model.encode(image_tensor)\n",
    "    image_embeds = info[2]  # Extract the embeddings\n",
    "    print(\"Shape: \", image_embeds.shape, \"Type: \", image_embeds.type)\n",
    "\n",
    "image_embeds.shape"
   ],
   "id": "dfbd8a772de4a192"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:49:45.320103Z",
     "start_time": "2025-04-23T21:49:44.159301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "from einops import rearrange\n",
    "\n",
    "image_path = \"generated_samples/img_0.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize to the required input size\n",
    "#     transforms.ToTensor(),  # Convert to a tensor\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "# ])\n",
    "\n",
    "# image_tensor = preprocess(image).unsqueeze(0).to(torch.bfloat16).to(\"cuda:0\")\n",
    "\n",
    "images_outputs = vl_chat_processor.image_processor([image], return_tensors=\"pt\")\n",
    "\n",
    "prompt = \"<image_placeholder>\"\n",
    "\n",
    "input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "input_ids = torch.LongTensor(input_ids)\n",
    "\n",
    "image_token_mask: torch.BoolTensor = input_ids == vl_chat_processor.image_id\n",
    "image_indices = image_token_mask.nonzero()\n",
    "\n",
    "input_ids, num_image_tokens = vl_chat_processor.add_image_token(\n",
    "    image_indices=image_indices,\n",
    "    input_ids=input_ids,\n",
    ")\n",
    "\n",
    "prepare = VLChatProcessorOutput(\n",
    "    sft_format=prompt,\n",
    "    input_ids=input_ids,\n",
    "    pixel_values=images_outputs.pixel_values,\n",
    "    num_image_tokens=num_image_tokens,\n",
    ")\n",
    "input_ids, num_image_tokens = input_ids.cuda(), num_image_tokens.cuda()\n",
    "bs, n = images_outputs.pixel_values.unsqueeze(0).shape[0:2]\n",
    "images = rearrange(images_outputs.pixel_values.unsqueeze(0).to(torch.bfloat16).to(\"cuda:0\"), \"b n c h w -> (b n) c h w\")\n",
    "images_embeds = vl_gpt.aligner(vl_gpt.vision_model(images))\n",
    "images_embeds = rearrange(images_embeds, \"(b n) t d -> b (n t) d\", b=bs, n=n)\n",
    "images_embeds.shape\n"
   ],
   "id": "18ef8a72b31be9f0",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vl_gpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 39\u001B[0m\n\u001B[1;32m     37\u001B[0m bs, n \u001B[38;5;241m=\u001B[39m images_outputs\u001B[38;5;241m.\u001B[39mpixel_values\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m     38\u001B[0m images \u001B[38;5;241m=\u001B[39m rearrange(images_outputs\u001B[38;5;241m.\u001B[39mpixel_values\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mbfloat16)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb n c h w -> (b n) c h w\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 39\u001B[0m images_embeds \u001B[38;5;241m=\u001B[39m \u001B[43mvl_gpt\u001B[49m\u001B[38;5;241m.\u001B[39maligner(vl_gpt\u001B[38;5;241m.\u001B[39mvision_model(images))\n\u001B[1;32m     40\u001B[0m images_embeds \u001B[38;5;241m=\u001B[39m rearrange(images_embeds, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(b n) t d -> b (n t) d\u001B[39m\u001B[38;5;124m\"\u001B[39m, b\u001B[38;5;241m=\u001B[39mbs, n\u001B[38;5;241m=\u001B[39mn)\n\u001B[1;32m     41\u001B[0m images_embeds\u001B[38;5;241m.\u001B[39mshape\n",
      "\u001B[0;31mNameError\u001B[0m: name 'vl_gpt' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:42:45.063027Z",
     "start_time": "2025-04-23T20:42:45.052689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temp = image_token_mask.detach().cpu()\n",
    "# get the indices of the True values\n",
    "indices = torch.nonzero(temp, as_tuple=True)[0]\n",
    "num_image_tokens"
   ],
   "id": "ea2263f651541186",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([576], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:43:16.567728Z",
     "start_time": "2025-04-23T20:43:10.847130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from janus.utils.io import load_pil_images\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"User\",\n",
    "        \"content\": \"<image_placeholder>\",\n",
    "        \"images\": [\"generated_samples/img_0.jpg\"],\n",
    "    },\n",
    "    {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "# load images and prepare for inputs\n",
    "pil_images = load_pil_images(conversation)\n",
    "prepare_inputs = vl_chat_processor(\n",
    "    conversations=conversation, images=pil_images, force_batchify=True\n",
    ").to(vl_gpt.device)\n",
    "\n",
    "inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)"
   ],
   "id": "8ca3915ee752f016",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:43:49.631538Z",
     "start_time": "2025-04-23T20:43:49.622207Z"
    }
   },
   "cell_type": "code",
   "source": "inputs_embeds.shape",
   "id": "2856c482e6e8ad51",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 623, 2048])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:07:22.029379Z",
     "start_time": "2025-04-23T21:07:21.853273Z"
    }
   },
   "cell_type": "code",
   "source": "prepare = vl_chat_processor.process_one(prompt=\"<image_placeholder>\", images=[image, image])",
   "id": "22beefd56fea04c2",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:08:41.364082Z",
     "start_time": "2025-04-23T21:08:41.341839Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d580846c8974bfcd",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:08:44.133487Z",
     "start_time": "2025-04-23T21:08:44.115377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "images_outputs = vl_chat_processor.image_processor([image, image], return_tensors=\"pt\")\n",
    "images_outputs[\"pixel_values\"].shape"
   ],
   "id": "9c94b17ad1af1ad8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 384, 384])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:08:48.558822Z",
     "start_time": "2025-04-23T21:08:47.246489Z"
    }
   },
   "cell_type": "code",
   "source": "res = get_image_janus_embeds([image, image])",
   "id": "4cf28dfb5f9e732e",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:08:48.575879Z",
     "start_time": "2025-04-23T21:08:48.569475Z"
    }
   },
   "cell_type": "code",
   "source": "res.shape",
   "id": "6fdf4803fe21cf97",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 576, 2048])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:43:57.419864Z",
     "start_time": "2025-04-23T20:43:57.016662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "prompt = \"<image_placeholder>\"\n",
    "\n",
    "input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "input_ids = torch.LongTensor(input_ids)\n",
    "\n",
    "image_token_mask: torch.BoolTensor = input_ids == vl_chat_processor.image_id\n",
    "image_indices = image_token_mask.nonzero()\n",
    "\n",
    "input_ids, num_image_tokens = input_ids, num_image_tokens\n",
    "\n",
    "input_ids, num_image_tokens = vl_chat_processor.add_image_token(\n",
    "    image_indices=image_indices,\n",
    "    input_ids=input_ids,\n",
    ")\n",
    "images_outputs = vl_chat_processor.image_processor([image], return_tensors=\"pt\")\n",
    "\n",
    "prepare = VLChatProcessorOutput(\n",
    "    sft_format=prompt,\n",
    "    input_ids=input_ids,\n",
    "    pixel_values=images_outputs.pixel_values.cuda().to(torch.bfloat16),\n",
    "    num_image_tokens=num_image_tokens,\n",
    ")\n",
    "\n",
    "batch_prepare = vl_chat_processor.batchify([prepare])\n",
    "\n",
    "bs, n = images_outputs.pixel_values.unsqueeze(0).shape[0:2]\n",
    "images = rearrange(images_outputs.pixel_values.unsqueeze(0).to(torch.bfloat16).to(\"cuda:0\"), \"b n c h w -> (b n) c h w\")\n",
    "images_embeds = vl_gpt.aligner(vl_gpt.vision_model(images))\n",
    "images_embeds = rearrange(images_embeds, \"(b n) t d -> b (n t) d\", b=bs, n=n)\n",
    "images_embeds.shape"
   ],
   "id": "10f873aea4461a2e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 576, 2048])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T07:36:27.757732Z",
     "start_time": "2025-04-22T07:36:27.748894Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6])\n",
      "torch.Size([6, 2048])\n"
     ]
    }
   ],
   "execution_count": 12,
   "source": [
    "print(input_ids.shape)\n",
    "inputs_embeds = vl_gpt.language_model.get_input_embeddings()(input_ids.to(\"cuda:0\"))\n",
    "print(inputs_embeds.shape)"
   ],
   "id": "6e36a4ad0f449100"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T21:06:54.180742Z",
     "start_time": "2025-04-21T21:06:54.110825Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"b n t -> b (n t)\".\n Input tensor shape: torch.Size([1]). Additional info: {}.\n Wrong shape: expected 3 dims. Received 1-dim tensor.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mEinopsError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/einops/einops.py:531\u001B[0m, in \u001B[0;36mreduce\u001B[0;34m(tensor, pattern, reduction, **axes_lengths)\u001B[0m\n\u001B[1;32m    530\u001B[0m shape \u001B[38;5;241m=\u001B[39m backend\u001B[38;5;241m.\u001B[39mshape(tensor)\n\u001B[0;32m--> 531\u001B[0m recipe \u001B[38;5;241m=\u001B[39m \u001B[43m_prepare_transformation_recipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxes_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43maxes_lengths\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mndim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _apply_recipe(\n\u001B[1;32m    533\u001B[0m     backend, recipe, cast(Tensor, tensor), reduction_type\u001B[38;5;241m=\u001B[39mreduction, axes_lengths\u001B[38;5;241m=\u001B[39mhashable_axes_lengths\n\u001B[1;32m    534\u001B[0m )\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/einops/einops.py:366\u001B[0m, in \u001B[0;36m_prepare_transformation_recipe\u001B[0;34m(pattern, operation, axes_names, ndim)\u001B[0m\n\u001B[1;32m    365\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ndim \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(left\u001B[38;5;241m.\u001B[39mcomposition):\n\u001B[0;32m--> 366\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m EinopsError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWrong shape: expected \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(left\u001B[38;5;241m.\u001B[39mcomposition)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m dims. Received \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-dim tensor.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    367\u001B[0m left_composition \u001B[38;5;241m=\u001B[39m left\u001B[38;5;241m.\u001B[39mcomposition\n",
      "\u001B[0;31mEinopsError\u001B[0m: Wrong shape: expected 3 dims. Received 1-dim tensor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mEinopsError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m images_emb_mask \u001B[38;5;241m=\u001B[39m \u001B[43mrearrange\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_image_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mb n t -> b (n t)\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/einops/einops.py:600\u001B[0m, in \u001B[0;36mrearrange\u001B[0;34m(tensor, pattern, **axes_lengths)\u001B[0m\n\u001B[1;32m    545\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrearrange\u001B[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39maxes_lengths: Size) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    546\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    547\u001B[0m \u001B[38;5;124;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001B[39;00m\n\u001B[1;32m    548\u001B[0m \u001B[38;5;124;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    598\u001B[0m \n\u001B[1;32m    599\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 600\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrearrange\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43maxes_lengths\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/einops/einops.py:542\u001B[0m, in \u001B[0;36mreduce\u001B[0;34m(tensor, pattern, reduction, **axes_lengths)\u001B[0m\n\u001B[1;32m    540\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m Input is list. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    541\u001B[0m message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAdditional info: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(axes_lengths)\n\u001B[0;32m--> 542\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m EinopsError(message \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(e))\n",
      "\u001B[0;31mEinopsError\u001B[0m:  Error while processing rearrange-reduction pattern \"b n t -> b (n t)\".\n Input tensor shape: torch.Size([1]). Additional info: {}.\n Wrong shape: expected 3 dims. Received 1-dim tensor."
     ]
    }
   ],
   "execution_count": 40,
   "source": "images_emb_mask = rearrange(num_image_tokens, \"b n t -> b (n t)\")",
   "id": "47cd9ff9ff39ee7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T21:09:57.586444Z",
     "start_time": "2025-04-21T21:09:57.579814Z"
    }
   },
   "cell_type": "code",
   "source": "inputs_embeds.shape",
   "id": "eb0bce08b3345bc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 623, 2048])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T21:48:42.461651Z",
     "start_time": "2025-04-21T21:48:42.334772Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ff1c5088f3dd6e17",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m images_embeds \u001B[38;5;241m=\u001B[39m vl_gpt\u001B[38;5;241m.\u001B[39maligner(\u001B[43mvl_gpt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvision_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/mnt/e/AI Talent Hub/MusesthAI/Janus/janus/models/clip_encoder.py:120\u001B[0m, in \u001B[0;36mCLIPVisionTower.forward\u001B[0;34m(self, images)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_norm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    118\u001B[0m     images \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_norm(images)\n\u001B[0;32m--> 120\u001B[0m image_forward_outs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvision_tower\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m image_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_select(image_forward_outs)\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m image_features\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/mnt/e/AI Talent Hub/MusesthAI/Janus/janus/models/siglip_vit.py:586\u001B[0m, in \u001B[0;36mVisionTransformer.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m--> 586\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    587\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_head:\n\u001B[1;32m    588\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_head(x)\n",
      "File \u001B[0;32m/mnt/e/AI Talent Hub/MusesthAI/Janus/janus/models/siglip_vit.py:563\u001B[0m, in \u001B[0;36mVisionTransformer.forward_features\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward_features\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m--> 563\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatch_embed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    564\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pos_embed(x)\n\u001B[1;32m    565\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_drop(x)\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.virtualenvs/janus-ubuntu/lib/python3.10/site-packages/timm/layers/patch_embed.py:113\u001B[0m, in \u001B[0;36mPatchEmbed.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 113\u001B[0m     B, C, H, W \u001B[38;5;241m=\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    115\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrict_img_size:\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Image' object has no attribute 'shape'"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pad_tokens = torch.zeros((1, len(text_embeds) + len(image_embeds) + len(audio_proj)), dtype=torch.int)\n",
    "pad_tokens[1, 1:-1] = vl_chat_processor.pad_id\n",
    "pad_embeds = vl_gpt.language_model.get_input_embeddings()(pad_tokens)\n",
    "input_embeds = torch.concat([text_embeds, image_embeds, audio_proj], dim=1)\n",
    "input_embeds = torch.stack([input_embeds, pad_embeds])"
   ],
   "id": "3e4ceddee4f98ed9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
